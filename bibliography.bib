
@misc{ardizzone_guided_2019,
	title = {Guided {Image} {Generation} with {Conditional} {Invertible} {Neural} {Networks}},
	author = {Ardizzone, Lynton and Lüth, Carsten and Kruse, Jakob and Rother, Carsten and Köthe, Ullrich},
	year = {2019},
	note = {\_eprint: 1907.02392},
}

@article{barnard_modelling_2000,
	title = {Modelling {Covariance} {Matrices} in {Terms} of {Standard} {Deviations} and {Correlations}, with {Application} {To} {Shrinkage}},
	volume = {10},
	journal = {Statistica Sinica},
	author = {Barnard, John and McCulloch, Robert and Meng, Xiao-Li},
	month = oct,
	year = {2000},
	pages = {1281--1311},
}

@article{beaumont_approximate_2002,
	title = {Approximate {Bayesian} {Computation} in {Population} {Genetics}},
	volume = {162},
	doi = {10.1093/genetics/162.4.2025},
	number = {4},
	journal = {Genetics},
	author = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
	month = dec,
	year = {2002},
	note = {Publisher: Oxford University Press (OUP)},
	pages = {2025--2035},
}

@book{berger_statistical_1985,
	address = {New York},
	title = {Statistical decision theory and {Bayesian} analysis},
	isbn = {0-387-96098-8},
	publisher = {Springer-Verlag},
	author = {Berger, James},
	year = {1985},
}

@article{betancourt_conceptual_2017,
	title = {A conceptual introduction to {Hamiltonian} {Monte} {Carlo}},
	journal = {arXiv preprint},
	author = {Betancourt, Michael},
	year = {2017},
}

@inproceedings{denker_transforming_1991,
	title = {Transforming {Neural}-{Net} {Output} {Levels} to {Probability} {Distributions}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 3},
	publisher = {Morgan Kaufmann},
	author = {Denker, John and Lecun, Yann},
	year = {1991},
	pages = {853--859},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{nowak_entropy-based_2016,
	title = {Entropy-based experimental design for optimal model discrimination in the geosciences},
	volume = {18},
	number = {11},
	journal = {Entropy},
	author = {Nowak, Wolfgang and Guthke, Anneli},
	year = {2016},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {409--434},
}

@article{schoniger_finding_2015,
	title = {Finding the right balance between groundwater model complexity and experimental effort via {Bayesian} model selection},
	volume = {531},
	journal = {Journal of Hydrology},
	author = {Schöniger, Anneli and Illman, Walter A and Wöhling, Thomas and Nowak, Wolfgang},
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {96--110},
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	journal = {The Journal of Machine Learning Research},
	author = {Gretton, A and Borgwardt, K. and Rasch, Malte and Schölkopf, Bernhard and Smola, AJ},
	month = mar,
	year = {2012},
	pages = {723--773},
}

@book{jaynes_probability_2003,
	address = {Cambridge},
	title = {Probability {Theory} – {The} {Logic} of {Science}},
	publisher = {Cambridge University Press},
	author = {Jaynes, E. T.},
	year = {2003},
}

@misc{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	author = {Kendall, Alex and Gal, Yarin},
	year = {2017},
	note = {\_eprint: 1703.04977},
}

@inproceedings{kingma_glow_2018,
	title = {Glow: {Generative} {Flow} with {Invertible} 1x1 {Convolutions}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Dhariwal, Prafulla},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@book{lehmann_theory_1998,
	address = {New York},
	title = {Theory of point estimation},
	isbn = {0-387-98502-6},
	publisher = {Springer},
	author = {Lehmann, E. L.},
	year = {1998},
}

@book{mardia_multivariate_1979,
	address = {London},
	series = {Probability and mathematical statistics},
	title = {Multivariate analysis},
	publisher = {Acad. Press},
	author = {Mardia, Kantilal Varichand and Kent, John T. and Bibby, John M.},
	year = {1979},
	keywords = {Multivariate\_analysis},
}

@book{mcelreath_statistical_2016,
	title = {Statistical {Rethinking} – {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	publisher = {CRC Press/Taylor \& Francis Group},
	author = {McElreath, Richard},
	year = {2016},
}

@article{muandet_kernel_2017,
	title = {Kernel {Mean} {Embedding} of {Distributions}: {A} {Review} and {Beyond}},
	volume = {10},
	doi = {10.1561/2200000060},
	number = {1-2},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	note = {Publisher: Now Publishers},
	pages = {1--141},
}

@misc{murphy_conjugate_2007,
	title = {Conjugate {Bayesian} analysis of the {Gaussian} distribution},
	author = {Murphy, Kevin},
	month = nov,
	year = {2007},
}

@book{oshana_dsp_2006,
	title = {{DSP} {Software} {Development} {Techniques} for {Embedded} and {Real}-{Time} {Systems}},
	publisher = {Elsevier},
	author = {Oshana, Robert},
	year = {2006},
	doi = {10.1016/b978-0-7506-7759-2.x5000-5},
}

@article{pritchard_population_1999,
	title = {Population growth of human {Y} chromosomes: a study of {Y} chromosome microsatellites},
	volume = {16},
	doi = {10.1093/oxfordjournals.molbev.a026091},
	number = {12},
	journal = {Molecular Biology and Evolution},
	author = {Pritchard, J. K. and Seielstad, M. T. and Perez-Lezaun, A. and Feldman, M. W.},
	year = {1999},
	note = {Publisher: Oxford University Press (OUP)},
	pages = {1791--1798},
}

@article{voss_interpreting_2004,
	title = {Interpreting the parameters of the diffusion model: {An} empirical validation},
	volume = {32},
	doi = {10.3758/bf03196893},
	number = {7},
	journal = {Memory \& Cognition},
	author = {Voss, Andreas and Rothermund, Klaus and Voss, Jochen},
	month = oct,
	year = {2004},
	note = {Publisher: Springer Science and Business Media LLC},
	pages = {1206--1220},
}

@article{radev_towards_2019,
	title = {Towards end-to-end likelihood-free inference with convolutional neural networks},
	volume = {73},
	doi = {10.1111/bmsp.12159},
	number = {1},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Radev, Stefan T. and Mertens, Ulf K. and Voss, Andreas and Köthe, Ullrich},
	month = feb,
	year = {2019},
	note = {Publisher: Wiley},
	pages = {23--43},
}

@article{radev_bayesflow_2020,
	title = {{BayesFlow}: {Learning} complex stochastic models with invertible neural networks},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Radev, Stefan T and Mertens, Ulf K and Voss, Andreas and Ardizzone, Lynton and Köthe, Ullrich},
	year = {2020},
	note = {Publisher: IEEE},
}

@book{raiffa_applied_1961,
	series = {Harvard {Business} {School} {Publications}},
	title = {Applied {Statistical} {Decision} {Theory}},
	publisher = {Division of Research, Graduate School of Business Adminitration, Harvard University},
	author = {Raiffa, H. and Raiffa, F.P.R.P.M.E.H. and Schlaifer, R.},
	year = {1961},
}

@article{ratcliff_diffusion_2008,
	title = {The {Diffusion} {Decision} {Model}: {Theory} and {Data} for {Two}-{Choice} {Decision} {Tasks}},
	volume = {20},
	doi = {10.1162/neco.2008.12-06-420},
	number = {4},
	journal = {Neural Computation},
	author = {Ratcliff, Roger and McKoon, Gail},
	month = apr,
	year = {2008},
	note = {Publisher: MIT Press - Journals},
	pages = {873--922},
}

@misc{rustamov_closed-form_2020,
	title = {Closed-form {Expressions} for {Maximum} {Mean} {Discrepancy} with {Applications} to {Wasserstein} {Auto}-{Encoders}},
	author = {Rustamov, Raif M.},
	year = {2020},
	note = {\_eprint: 1901.03227},
}

@article{tavare_inferring_1997,
	title = {Inferring {Coalescence} {Times} {From} {DNA} {Sequence} {Data}},
	volume = {145},
	issn = {0016-6731},
	number = {2},
	journal = {Genetics},
	author = {Tavaré, Simon and Balding, David J. and Griffiths, R. C. and Donnelly, Peter},
	year = {1997},
	note = {Publisher: Genetics},
	pages = {505--518},
}

@misc{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	note = {\_eprint: 1603.04467},
}

@article{timmer_generating_1995,
	title = {On generating power law noise},
	volume = {300},
	journal = {Astronomy and Astrophysics},
	author = {Timmer, J. and Koenig, M.},
	year = {1995},
	pages = {707--710},
}

@article{ratcliff_connectionist_1999,
	title = {Connectionist and diffusion models of reaction time.},
	volume = {106},
	number = {2},
	journal = {Psychological review},
	author = {Ratcliff, Roger and Van Zandt, Trisha and McKoon, Gail},
	year = {1999},
	note = {Publisher: American Psychological Association},
	pages = {261},
}

@incollection{brauer_compartmental_2008,
	title = {Compartmental models in epidemiology},
	booktitle = {Mathematical epidemiology},
	publisher = {Springer},
	author = {Brauer, Fred},
	year = {2008},
	pages = {19--79},
}

@article{shiono_estimation_2021,
	title = {Estimation of agent-based models using {Bayesian} deep learning approach of {BayesFlow}},
	volume = {125},
	journal = {Journal of Economic Dynamics and Control},
	author = {Shiono, Takashi},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {104082},
}

@article{bieringer_measuring_2021,
	title = {Measuring {QCD} {Splittings} with {Invertible} {Networks}},
	volume = {10},
	number = {6},
	journal = {SciPost Physics Proceedings},
	author = {Bieringer, Sebastian and Butter, Anja and Heimel, Theo and Höche, Stefan and Köthe, Ullrich and Plehn, Tilman and Radev, Stefan T},
	year = {2021},
	note = {Publisher: Stichting SciPost},
}

@article{ardizzone_analyzing_2018,
	title = {Analyzing inverse problems with invertible neural networks},
	journal = {arXiv preprint arXiv:1808.04730},
	author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W and Klessen, Ralf S and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
	year = {2018},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	note = {Publisher: Wiley Online Library},
	pages = {389--402},
}

@inproceedings{greenberg_automatic_2019,
	title = {Automatic posterior transformation for likelihood-free inference},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
	year = {2019},
	pages = {2404--2414},
}

@article{radev_outbreakflow_2021,
	title = {{OutbreakFlow}: {Model}-based {Bayesian} inference of disease outbreak dynamics with invertible neural networks and its application to the {COVID}-19 pandemics in {Germany}},
	volume = {17},
	number = {10},
	journal = {PLOS Computational Biology},
	author = {Radev, Stefan T and Graw, Frederik and Chen, Simiao and Mutters, Nico T and Eichel, Vanessa M and Bärnighausen, Till and Köthe, Ullrich},
	year = {2021},
	note = {Publisher: Public Library of Science San Francisco, CA USA},
	pages = {e1009472},
}

@article{carpenter_stan_2017,
	title = {Stan: {A} probabilistic programming language},
	volume = {76},
	number = {1},
	journal = {Journal of statistical software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017},
	note = {Publisher: Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)},
}

@misc{stan_development_team_stan_2018,
	title = {The {Stan} {Core} {Library}},
	url = {http://mc-stan.org/ 5},
	author = {{Stan Development Team}},
	year = {2018},
}

@article{dehning_inferring_2020,
	title = {Inferring change points in the spread of {COVID}-19 reveals the effectiveness of interventions},
	volume = {369},
	number = {6500},
	journal = {Science},
	author = {Dehning, Jonas and Zierenberg, Johannes and Spitzner, F Paul and Wibral, Michael and Neto, Joao Pinheiro and Wilczek, Michael and Priesemann, Viola},
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
}

@article{thomas_diagnosing_2019,
	title = {Diagnosing model misspecification and performing generalized {Bayes}' updates via probabilistic classifiers},
	journal = {arXiv preprint arXiv:1912.05810},
	author = {Thomas, Owen and Corander, Jukka},
	year = {2019},
}

@inproceedings{durkan_contrastive_2020,
	title = {On contrastive learning for likelihood-free inference},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Durkan, Conor and Murray, Iain and Papamakarios, George},
	year = {2020},
	pages = {2771--2781},
}

@article{lueckmann_flexible_2017,
	title = {Flexible statistical inference for mechanistic models of neural dynamics},
	volume = {30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Lueckmann, Jan-Matthis and Goncalves, Pedro J and Bassetto, Giacomo and Öcal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H},
	year = {2017},
}

@article{grunwald_inconsistency_2017,
	title = {Inconsistency of {Bayesian} inference for misspecified linear models, and a proposal for repairing it},
	volume = {12},
	number = {4},
	journal = {Bayesian Analysis},
	author = {Grünwald, Peter and Van Ommen, Thijs and {others}},
	year = {2017},
	note = {Publisher: International Society for Bayesian Analysis},
	pages = {1069--1103},
}

@article{gers_learning_2000,
	title = {Learning to forget: {Continual} prediction with {LSTM}},
	volume = {12},
	number = {10},
	journal = {Neural computation},
	author = {Gers, Felix A and Schmidhuber, Jürgen and Cummins, Fred},
	year = {2000},
	note = {Publisher: MIT Press},
	pages = {2451--2471},
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
}

@article{bloem-reddy_probabilistic_2020,
	title = {Probabilistic {Symmetries} and {Invariant} {Neural} {Networks}.},
	volume = {21},
	journal = {J. Mach. Learn. Res.},
	author = {Bloem-Reddy, Benjamin and Teh, Yee Whye},
	year = {2020},
	pages = {90--1},
}

@article{cranmer_frontier_2020,
	title = {The frontier of simulation-based inference},
	volume = {117},
	number = {48},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	year = {2020},
	note = {Publisher: National Acad Sciences},
	pages = {30055--30062},
}

@inproceedings{papamakarios_sequential_2019,
	title = {Sequential neural likelihood: {Fast} likelihood-free inference with autoregressive flows},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Papamakarios, George and Sterratt, David and Murray, Iain},
	year = {2019},
	pages = {837--848},
}

@article{hoffman_no-u-turn_2014,
	title = {The {No}-{U}-{Turn} sampler: adaptively setting path lengths in {Hamiltonian} {Monte} {Carlo}.},
	volume = {15},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D and Gelman, Andrew},
	year = {2014},
	pages = {1593--1623},
}

@article{wiqvist_sequential_2021,
	title = {Sequential {Neural} {Posterior} and {Likelihood} {Approximation}},
	journal = {arXiv preprint arXiv:2102.06522},
	author = {Wiqvist, Samuel and Frellsen, Jes and Picchini, Umberto},
	year = {2021},
}

@inproceedings{hermans_likelihood-free_2020,
	title = {Likelihood-free mcmc with amortized approximate ratio estimators},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
	year = {2020},
	pages = {4239--4248},
}

@article{radev_amortized_2021,
	title = {Amortized bayesian model comparison with evidential deep learning},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Radev, Stefan T and D'Alessandro, Marco and Mertens, Ulf K and Voss, Andreas and Köthe, Ullrich and Bürkner, Paul-Christian},
	year = {2021},
	note = {Publisher: IEEE},
}

@misc{radev_amortized_2020,
	title = {Amortized {Bayesian} {Inference} for {Models} of {Cognition}},
	author = {Radev, Stefan T. and Voss, Andreas and Wieschen, Eva Marie and Buerkner, Paul-Christian},
	year = {2020},
	note = {\_eprint: arXiv:2005.03899},
}

@article{stine_introduction_1989,
	title = {An {Introduction} to {Bootstrap} {Methods}},
	volume = {18},
	url = {https://doi.org/10.1177/0049124189018002003},
	doi = {10.1177/0049124189018002003},
	number = {2-3},
	journal = {Sociological Methods \& Research},
	author = {Stine, Robert},
	month = nov,
	year = {1989},
	note = {Publisher: SAGE Publications},
	pages = {243--291},
}

@article{arvanitidis_pulling_2021,
	title = {Pulling back information geometry},
	journal = {arXiv preprint arXiv:2106.05367},
	author = {Arvanitidis, Georgios and González-Duque, Miguel and Pouplin, Alison and Kalatzis, Dimitris and Hauberg, Søren},
	year = {2021},
}

@article{goncalves_training_2020,
	title = {Training deep neural density estimators to identify mechanistic models of neural dynamics},
	volume = {9},
	journal = {Elife},
	author = {Gonçalves, Pedro J and Lueckmann, Jan-Matthis and Deistler, Michael and Nonnenmacher, Marcel and Öcal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F and Haddad, Sara A and Vogels, Tim P and {others}},
	year = {2020},
	note = {Publisher: eLife Sciences Publications Limited},
	pages = {e56261},
}

@article{hullermeier_aleatoric_2021,
	title = {Aleatoric and {Epistemic} {Uncertainty} in {Machine} {Learning}: {An} {Introduction} to {Concepts} and {Methods}},
	volume = {110},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Aleatoric and {Epistemic} {Uncertainty} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09457},
	doi = {10.1007/s10994-021-05946-3},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	number = {3},
	urldate = {2022-01-22},
	journal = {Machine Learning},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	month = mar,
	year = {2021},
	note = {arXiv: 1910.09457},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {457--506},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/G6HT78WF/Hüllermeier and Waegeman - 2021 - Aleatoric and Epistemic Uncertainty in Machine Lea.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/3TJS9WZG/1910.html:text/html},
}

@article{oelrich_when_2020,
	title = {When are {Bayesian} model probabilities overconfident?},
	url = {http://arxiv.org/abs/2003.04026},
	abstract = {Bayesian model comparison is often based on the posterior distribution over the set of compared models. This distribution is often observed to concentrate on a single model even when other measures of model fit or forecasting ability indicate no strong preference. Furthermore, a moderate change in the data sample can easily shift the posterior model probabilities to concentrate on another model. We document overconfidence in two high-profile applications in economics and neuroscience. To shed more light on the sources of overconfidence we derive the sampling variance of the Bayes factor in univariate and multivariate linear regression. The results show that overconfidence is likely to happen when i) the compared models give very different approximations of the data-generating process, ii) the models are very flexible with large degrees of freedom that are not shared between the models, and iii) the models underestimate the true variability in the data.},
	urldate = {2022-01-22},
	journal = {arXiv:2003.04026 [math, stat]},
	author = {Oelrich, Oscar and Ding, Shutong and Magnusson, Måns and Vehtari, Aki and Villani, Mattias},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.04026},
	keywords = {Mathematics - Statistics Theory},
	file = {Oelrich et al. - 2020 - When are Bayesian model probabilities overconfiden.pdf:/Users/marvin/Zotero/storage/78CWBHPX/Oelrich et al. - 2020 - When are Bayesian model probabilities overconfiden.pdf:application/pdf},
}

@article{burkner_brms_2017,
	title = {brms: {An} {R} {Package} for {Bayesian} {Multilevel} {Models} {Using} {Stan}},
	volume = {80},
	doi = {10.18637/jss.v080.i01},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2017},
	pages = {1--28},
}

@book{bishop_pattern_2016,
	address = {New York, NY},
	edition = {Softcover reprint of the original 1st edition 2006 (corrected at 8th printing 2009)},
	series = {Information {Science} and {Statistics}},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {978-1-4939-3843-8},
	language = {eng},
	publisher = {Springer New York},
	author = {Bishop, Christopher M.},
	year = {2016},
}

@article{gronau_bridgesampling_2020,
	title = {bridgesampling: {An} {R} {Package} for {Estimating} {Normalizing} {Constants}},
	volume = {92},
	issn = {1548-7660},
	shorttitle = {\textbf{bridgesampling}},
	url = {http://www.jstatsoft.org/v92/i10/},
	doi = {10.18637/jss.v092.i10},
	language = {en},
	number = {10},
	urldate = {2022-01-24},
	journal = {Journal of Statistical Software},
	author = {Gronau, Quentin F. and Singmann, Henrik and Wagenmakers, Eric-Jan},
	year = {2020},
	file = {Full Text:/Users/marvin/Zotero/storage/79MKCGL3/Gronau et al. - 2020 - bridgesampling  An R Package for Es.pdf:application/pdf},
}

@article{meng_warp_2002,
	title = {Warp {Bridge} {Sampling}},
	volume = {11},
	number = {3},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Meng, Xiao-Li and Schilling, Stephen},
	year = {2002},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	pages = {552--586},
}

@article{schad_workflow_2021,
	title = {Workflow {Techniques} for the {Robust} {Use} of {Bayes} {Factors}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2103.08744},
	doi = {10.48550/ARXIV.2103.08744},
	abstract = {Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes factors provide one general way to compare different hypotheses by their compatibility with the observed data. Those quantifications can then also be used to choose between hypotheses. While Bayes factors provide an immediate approach to hypothesis testing, they are highly sensitive to details of the data/model assumptions. Moreover it's not clear how straightforwardly this approach can be implemented in practice, and in particular how sensitive it is to the details of the computational implementation. Here, we investigate these questions for Bayes factor analyses in the cognitive sciences. We explain the statistics underlying Bayes factors as a tool for Bayesian inferences and discuss that utility functions are needed for principled decisions on hypotheses. Next, we study how Bayes factors misbehave under different conditions. This includes a study of errors in the estimation of Bayes factors. Importantly, it is unknown whether Bayes factor estimates based on bridge sampling are unbiased for complex analyses. We are the first to use simulation-based calibration as a tool to test the accuracy of Bayes factor estimates. Moreover, we study how stable Bayes factors are against different MCMC draws. We moreover study how Bayes factors depend on variation in the data. We also look at variability of decisions based on Bayes factors and how to optimize decisions using a utility function. We outline a Bayes factor workflow that researchers can use to study whether Bayes factors are robust for their individual analysis, and we illustrate this workflow using an example from the cognitive sciences. We hope that this study will provide a workflow to test the strengths and limitations of Bayes factors as a way to quantify evidence in support of scientific hypotheses. Reproducible code is available from https://osf.io/y354c/.},
	urldate = {2022-02-24},
	author = {Schad, Daniel J. and Nicenboim, Bruno and Bürkner, Paul-Christian and Betancourt, Michael and Vasishth, Shravan},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Methodology (stat.ME)},
	file = {Schad et al. - 2021 - Workflow Techniques for the Robust Use of Bayes Fa.pdf:/Users/marvin/Zotero/storage/ABXGUB4G/Schad et al. - 2021 - Workflow Techniques for the Robust Use of Bayes Fa.pdf:application/pdf},
}

@article{watanabe_asymptotic_2010,
	title = {Asymptotic {Equivalence} of {Bayes} {Cross} {Validation} and {Widely} {Applicable} {Information} {Criterion} in {Singular} {Learning} {Theory}},
	volume = {11},
	abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the
Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we
established the singular learning theory and proposed a widely applicable information criterion, the
expectation value of which is asymptotically equal to the average Bayes generalization loss. In the
present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable
information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore,
model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is
asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of
training samples. Therefore the relation between the cross-validation error and the generalization
error is determined by the algebraic geometrical structure of a learning machine. We also clarify
that the deviance information criteria are different from the Bayes cross-validation and the widely
applicable information criterion},
	journal = {Journal of Machine Learning Research},
	author = {Watanabe, Sumio},
	year = {2010},
	pages = {3571--3594},
	file = {Watanabe - 2010 - Asymptotic Equivalence of Bayes Cross Validation a.pdf:/Users/marvin/Zotero/storage/WZMTGTP3/Watanabe - 2010 - Asymptotic Equivalence of Bayes Cross Validation a.pdf:application/pdf},
}

@article{stone_asymptotic_1977,
	title = {An {Asymptotic} {Equivalence} of {Choice} of {Model} by {Cross}-{Validation} and {Akaike}'s {Criterion}},
	volume = {39},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984877},
	abstract = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
	number = {1},
	urldate = {2022-04-06},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Stone, M.},
	year = {1977},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {44--47},
	file = {Stone - 1977 - An Asymptotic Equivalence of Choice of Model by Cr.pdf:/Users/marvin/Zotero/storage/2Q9DA4YY/Stone - 1977 - An Asymptotic Equivalence of Choice of Model by Cr.pdf:application/pdf},
}

@article{gronau_limitations_2019,
	title = {Limitations of {Bayesian} {Leave}-{One}-{Out} {Cross}-{Validation} for {Model} {Selection}},
	volume = {2},
	issn = {2522-0861, 2522-087X},
	url = {http://link.springer.com/10.1007/s42113-018-0011-7},
	doi = {10.1007/s42113-018-0011-7},
	language = {en},
	number = {1},
	urldate = {2022-04-06},
	journal = {Computational Brain \& Behavior},
	author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan},
	month = mar,
	year = {2019},
	pages = {1--11},
	file = {Full Text:/Users/marvin/Zotero/storage/E3PGCSFA/Gronau and Wagenmakers - 2019 - Limitations of Bayesian Leave-One-Out Cross-Valida.pdf:application/pdf;Gronau-Wagenmakers2019_Article_LimitationsOfBayesianLeave-One.pdf:/Users/marvin/Zotero/storage/IITJA2FF/Gronau-Wagenmakers2019_Article_LimitationsOfBayesianLeave-One.pdf:application/pdf},
}

@article{navarro_between_2019,
	title = {Between the {Devil} and the {Deep} {Blue} {Sea}: {Tensions} {Between} {Scientific} {Judgement} and {Statistical} {Model} {Selection}},
	volume = {2},
	issn = {2522-0861, 2522-087X},
	shorttitle = {Between the {Devil} and the {Deep} {Blue} {Sea}},
	url = {http://link.springer.com/10.1007/s42113-018-0019-z},
	doi = {10.1007/s42113-018-0019-z},
	language = {en},
	number = {1},
	urldate = {2022-04-06},
	journal = {Computational Brain \& Behavior},
	author = {Navarro, Danielle J.},
	month = mar,
	year = {2019},
	pages = {28--34},
	file = {0ef1566e657f34eba91a86fb0421c8f0.pdf:/Users/marvin/Zotero/storage/5AI4G8DT/0ef1566e657f34eba91a86fb0421c8f0.pdf:application/pdf;Submitted Version:/Users/marvin/Zotero/storage/WRBKNZIX/Navarro - 2019 - Between the Devil and the Deep Blue Sea Tensions .pdf:application/pdf},
}

@article{gu_approximated_2018,
	title = {Approximated adjusted fractional {Bayes} factors: {A} general method for testing informative hypotheses},
	volume = {71},
	issn = {00071102},
	shorttitle = {Approximated adjusted fractional {Bayes} factors},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bmsp.12110},
	doi = {10.1111/bmsp.12110},
	language = {en},
	number = {2},
	urldate = {2022-04-06},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Gu, Xin and Mulder, Joris and Hoijtink, Herbert},
	month = may,
	year = {2018},
	pages = {229--261},
	file = {Gu et al. - 2018 - Approximated adjusted fractional Bayes factors A .pdf:/Users/marvin/Zotero/storage/GTJCW4WZ/Gu et al. - 2018 - Approximated adjusted fractional Bayes factors A .pdf:application/pdf},
}

@article{ly_tutorial_2017,
	title = {A {Tutorial} on {Fisher} information},
	volume = {80},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249617301396},
	doi = {10.1016/j.jmp.2017.05.006},
	language = {en},
	urldate = {2022-04-06},
	journal = {Journal of Mathematical Psychology},
	author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul P.P.P. and Wagenmakers, Eric-Jan},
	month = oct,
	year = {2017},
	pages = {40--55},
	file = {Full Text:/Users/marvin/Zotero/storage/QZXYE7J9/Ly et al. - 2017 - A Tutorial on Fisher information.pdf:application/pdf;ly2017.pdf:/Users/marvin/Zotero/storage/IG3VREEN/ly2017.pdf:application/pdf},
}

@article{lodewyckx_tutorial_2011,
	title = {A tutorial on {Bayes} factor estimation with the product space method},
	volume = {55},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249611000423},
	doi = {10.1016/j.jmp.2011.06.001},
	language = {en},
	number = {5},
	urldate = {2022-04-06},
	journal = {Journal of Mathematical Psychology},
	author = {Lodewyckx, Tom and Kim, Woojae and Lee, Michael D. and Tuerlinckx, Francis and Kuppens, Peter and Wagenmakers, Eric-Jan},
	month = oct,
	year = {2011},
	pages = {331--347},
	file = {Full Text:/Users/marvin/Zotero/storage/YA7A54RZ/Lodewyckx et al. - 2011 - A tutorial on Bayes factor estimation with the pro.pdf:application/pdf;Lodewyckx2011ATOBF.pdf:/Users/marvin/Zotero/storage/2WZNFLVJ/Lodewyckx2011ATOBF.pdf:application/pdf},
}

@article{gronau_tutorial_2017,
	title = {A tutorial on bridge sampling},
	volume = {81},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249617300640},
	doi = {10.1016/j.jmp.2017.09.005},
	language = {en},
	urldate = {2022-04-06},
	journal = {Journal of Mathematical Psychology},
	author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
	month = dec,
	year = {2017},
	pages = {80--97},
	file = {Full Text:/Users/marvin/Zotero/storage/9U228MGN/Gronau et al. - 2017 - A tutorial on bridge sampling.pdf:application/pdf;Gronau et al. - 2017 - A tutorial on bridge sampling.pdf:/Users/marvin/Zotero/storage/BRWTYRUE/Gronau et al. - 2017 - A tutorial on bridge sampling.pdf:application/pdf},
}

@article{mulder_prior_2014,
	title = {Prior adjusted default {Bayes} factors for testing (in)equality constrained hypotheses},
	volume = {71},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947313002624},
	doi = {10.1016/j.csda.2013.07.017},
	language = {en},
	urldate = {2022-04-06},
	journal = {Computational Statistics \& Data Analysis},
	author = {Mulder, Joris},
	month = mar,
	year = {2014},
	pages = {448--463},
	file = {Mulder - 2014 - Prior adjusted default Bayes factors for testing (.pdf:/Users/marvin/Zotero/storage/IZ4YLITP/Mulder - 2014 - Prior adjusted default Bayes factors for testing (.pdf:application/pdf},
}

@article{conigliani_sensitivity_2000,
	title = {Sensitivity of the fractional {Bayes} factor to prior distributions},
	volume = {28},
	issn = {03195724, 1708945X},
	url = {http://doi.wiley.com/10.2307/3315983},
	doi = {10.2307/3315983},
	language = {en},
	number = {2},
	urldate = {2022-04-06},
	journal = {Canadian Journal of Statistics},
	author = {Conigliani, Caterina and O'hagan, Anthony},
	month = jun,
	year = {2000},
	pages = {343--352},
	file = {Conigliani and O'hagan - 2000 - Sensitivity of the fractional Bayes factor to prio.pdf:/Users/marvin/Zotero/storage/9SQ42A53/Conigliani and O'hagan - 2000 - Sensitivity of the fractional Bayes factor to prio.pdf:application/pdf},
}

@incollection{berger_objective_2001,
	address = {Beachwood, OH},
	title = {Objective {Bayesian} {Methods} for {Model} {Selection}: {Introduction} and {Comparison}},
	isbn = {978-0-940600-52-2},
	shorttitle = {Objective {Bayesian} {Methods} for {Model} {Selection}},
	url = {http://projecteuclid.org/euclid.lnms/1215540968},
	language = {en},
	urldate = {2022-04-06},
	booktitle = {Institute of {Mathematical} {Statistics} {Lecture} {Notes} - {Monograph} {Series}},
	publisher = {Institute of Mathematical Statistics},
	author = {Berger, James O. and Pericchi, Luis R.},
	year = {2001},
	doi = {10.1214/lnms/1215540968},
	pages = {135--207},
	file = {Berger and Pericchi - 2001 - Objective Bayesian Methods for Model Selection In.pdf:/Users/marvin/Zotero/storage/Q7RPJA9S/Berger and Pericchi - 2001 - Objective Bayesian Methods for Model Selection In.pdf:application/pdf},
}

@article{berger_intrinsic_1996,
	title = {The {Intrinsic} {Bayes} {Factor} for {Model} {Selection} and {Prediction}},
	volume = {91},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476668},
	doi = {10.1080/01621459.1996.10476668},
	language = {en},
	number = {433},
	urldate = {2022-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Berger, James O. and Pericchi, Luis R.},
	month = mar,
	year = {1996},
	pages = {109--122},
	file = {Berger and Pericchi - 1996 - The Intrinsic Bayes Factor for Model Selection and.pdf:/Users/marvin/Zotero/storage/UBVXXJX8/Berger and Pericchi - 1996 - The Intrinsic Bayes Factor for Model Selection and.pdf:application/pdf},
}

@article{wang_consistency_2016,
	title = {Consistency of {Bayes} factor for nonnested model selection when the model dimension grows},
	volume = {22},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-22/issue-4/Consistency-of-Bayes-factor-for-nonnested-model-selection-when-the/10.3150/15-BEJ720.full},
	doi = {10.3150/15-BEJ720},
	number = {4},
	urldate = {2022-04-06},
	journal = {Bernoulli},
	author = {Wang, Min and Maruyama, Yuzo},
	month = nov,
	year = {2016},
	file = {1503.06155.pdf:/Users/marvin/Zotero/storage/DCR4357Z/1503.06155.pdf:application/pdf;Full Text:/Users/marvin/Zotero/storage/BQDMAKIL/Wang and Maruyama - 2016 - Consistency of Bayes factor for nonnested model se.pdf:application/pdf},
}

@article{ohagan_fractional_1995,
	title = {Fractional {Bayes} {Factors} for {Model} {Comparison}},
	volume = {57},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1995.tb02017.x},
	doi = {10.1111/j.2517-6161.1995.tb02017.x},
	language = {en},
	number = {1},
	urldate = {2022-04-06},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {O'Hagan, Anthony},
	month = jan,
	year = {1995},
	pages = {99--118},
	file = {O'Hagan - 1995 - Fractional Bayes Factors for Model Comparison.pdf:/Users/marvin/Zotero/storage/7CZ6B5G4/O'Hagan - 1995 - Fractional Bayes Factors for Model Comparison.pdf:application/pdf},
}

@misc{berger_intrinsic_1994,
	title = {The {Intrinsic} {Bayes} {Factor} for {Linear} {Models}},
	author = {Berger, James O. and Pericchi, Luis R.},
	year = {1994},
	file = {Berger and Pericchi - 1994 - The Intrinsic Bayes Factor for Linear Models.pdf:/Users/marvin/Zotero/storage/KCLUTBI9/Berger and Pericchi - 1994 - The Intrinsic Bayes Factor for Linear Models.pdf:application/pdf},
}

@article{talts_validating_2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2022-04-06},
	journal = {arXiv:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1804.06788},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/LJGTEXMI/Talts et al. - 2020 - Validating Bayesian Inference Algorithms with Simu.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/CLDILBYU/1804.html:text/html},
}

@article{shao_linear_1993,
	title = {Linear {Model} {Selection} by {Cross}-validation},
	volume = {88},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476299},
	doi = {10.1080/01621459.1993.10476299},
	language = {en},
	number = {422},
	urldate = {2022-04-06},
	journal = {Journal of the American Statistical Association},
	author = {Shao, Jun},
	month = jun,
	year = {1993},
	pages = {486--494},
	file = {Shao - 1993 - Linear Model Selection by Cross-validation.pdf:/Users/marvin/Zotero/storage/R5PJEWV6/Shao - 1993 - Linear Model Selection by Cross-validation.pdf:application/pdf},
}

@book{ohagan_kendalls_1994,
	title = {Kendall's {Advanced} theory of statistics: {Volume} {2B}, {Bayesian} inference},
	publisher = {Edward Arnold},
	author = {O'Hagan, Anthony and Kendall, Maurice G. and Stuart, Alan},
	year = {1994},
	keywords = {Mathematical statistics},
	file = {Kendall et al. - 1994 - Kendall's advanced theory of statistics.pdf:/Users/marvin/Zotero/storage/BIKGAZD2/Kendall et al. - 1994 - Kendall's advanced theory of statistics.pdf:application/pdf},
}

@article{frazier_model_2020,
	title = {Model misspecification in approximate {Bayesian} computation: consequences and diagnostics},
	volume = {82},
	issn = {13697412},
	shorttitle = {Model misspecification in approximate {Bayesian} computation},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssb.12356},
	doi = {10.1111/rssb.12356},
	language = {en},
	number = {2},
	urldate = {2022-04-06},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Frazier, David T. and Robert, Christian P. and Rousseau, Judith},
	month = apr,
	year = {2020},
	pages = {421--444},
	file = {ABC_miss_FrRoRu-R2.pdf:/Users/marvin/Zotero/storage/BMRU8XNI/ABC_miss_FrRoRu-R2.pdf:application/pdf},
}

@article{frazier_robust_2021,
	title = {Robust {Approximate} {Bayesian} {Inference} {With} {Synthetic} {Likelihood}},
	volume = {30},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2021.1875839},
	doi = {10.1080/10618600.2021.1875839},
	language = {en},
	number = {4},
	urldate = {2022-04-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Frazier, David T. and Drovandi, Christopher},
	month = oct,
	year = {2021},
	pages = {958--976},
	file = {frazier2021.pdf:/Users/marvin/Zotero/storage/M97GM6E5/frazier2021.pdf:application/pdf},
}

@article{alquier_concentration_2019,
	title = {Concentration of tempered posteriors and of their variational approximations},
	url = {http://arxiv.org/abs/1706.09293},
	abstract = {While Bayesian methods are extremely popular in statistics and machine learning, their application to massive datasets is often challenging, when possible at all. Indeed, the classical MCMC algorithms are prohibitively slow when both the model dimension and the sample size are large. Variational Bayesian methods aim at approximating the posterior by a distribution in a tractable family. Thus, MCMC are replaced by an optimization algorithm which is orders of magnitude faster. VB methods have been applied in such computationally demanding applications as including collaborative filtering, image and video processing, NLP and text processing... However, despite very nice results in practice, the theoretical properties of these approximations are usually not known. In this paper, we propose a general approach to prove the concentration of variational approximations of fractional posteriors. We apply our theory to two examples: matrix completion, and Gaussian VB.},
	urldate = {2022-04-06},
	journal = {arXiv:1706.09293 [cs, math, stat]},
	author = {Alquier, Pierre and Ridgway, James},
	month = apr,
	year = {2019},
	note = {arXiv: 1706.09293},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
	file = {1706.09293.pdf:/Users/marvin/Zotero/storage/E8NR2IYW/1706.09293.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/Z2DJBLK3/1706.html:text/html},
}

@article{zhang_convergence_2020,
	title = {Convergence rates of variational posterior distributions},
	volume = {48},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-4/Convergence-rates-of-variational-posterior-distributions/10.1214/19-AOS1883.full},
	doi = {10.1214/19-AOS1883},
	number = {4},
	urldate = {2022-04-06},
	journal = {The Annals of Statistics},
	author = {Zhang, Fengshuo and Gao, Chao},
	month = aug,
	year = {2020},
	file = {10197522.pdf:/Users/marvin/Zotero/storage/TK9SR9LS/10197522.pdf:application/pdf;Submitted Version:/Users/marvin/Zotero/storage/CQTJ9QNG/Zhang and Gao - 2020 - Convergence rates of variational posterior distrib.pdf:application/pdf},
}

@article{martin_approximating_2021,
	title = {Approximating {Bayes} in the 21st {Century}},
	url = {http://arxiv.org/abs/2112.10342},
	abstract = {The 21st century has seen an enormous growth in the development and use of approximate Bayesian methods. Such methods produce computational solutions to certain intractable statistical problems that challenge exact methods like Markov chain Monte Carlo: for instance, models with unavailable likelihoods, high-dimensional models, and models featuring large data sets. These approximate methods are the subject of this review. The aim is to help new researchers in particular -- and more generally those interested in adopting a Bayesian approach to empirical work -- distinguish between different approximate techniques; understand the sense in which they are approximate; appreciate when and why particular methods are useful; and see the ways in which they can can be combined.},
	urldate = {2022-04-06},
	journal = {arXiv:2112.10342 [stat]},
	author = {Martin, Gael M. and Frazier, David T. and Robert, Christian P.},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.10342},
	keywords = {Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/9N4SFKXC/Martin et al. - 2021 - Approximating Bayes in the 21st Century.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/9XB2662D/2112.html:text/html},
}

@article{brynjarsdottir_learning_2014,
	title = {Learning about physical parameters: the importance of model discrepancy},
	volume = {30},
	issn = {0266-5611, 1361-6420},
	shorttitle = {Learning about physical parameters},
	url = {https://iopscience.iop.org/article/10.1088/0266-5611/30/11/114007},
	doi = {10.1088/0266-5611/30/11/114007},
	number = {11},
	urldate = {2022-04-06},
	journal = {Inverse Problems},
	author = {Brynjarsdóttir, Jenný and OʼHagan, Anthony},
	month = nov,
	year = {2014},
	pages = {114007},
	file = {Brynjarsdóttir and OʼHagan - 2014 - Learning about physical parameters the importance.pdf:/Users/marvin/Zotero/storage/5RKP4WSZ/Brynjarsdóttir and OʼHagan - 2014 - Learning about physical parameters the importance.pdf:application/pdf},
}

@article{bhattacharya_bayesian_2016,
	title = {Bayesian fractional posteriors},
	url = {http://arxiv.org/abs/1611.01125},
	abstract = {We consider the fractional posterior distribution that is obtained by updating a prior distribution via Bayes theorem with a fractional likelihood function, a usual likelihood function raised to a fractional power. First, we analyze the contraction property of the fractional posterior in a general misspecified framework. Our contraction results only require a prior mass condition on certain Kullback-Leibler (KL) neighborhood of the true parameter (or the KL divergence minimizer in the misspecified case), and obviate constructions of test functions and sieves commonly used in the literature for analyzing the contraction property of a regular posterior. We show through a counterexample that some condition controlling the complexity of the parameter space is necessary for the regular posterior to contract, rendering additional flexibility on the choice of the prior for the fractional posterior. Second, we derive a novel Bayesian oracle inequality based on a PAC-Bayes inequality in misspecified models. Our derivation reveals several advantages of averaging based Bayesian procedures over optimization based frequentist procedures. As an application of the Bayesian oracle inequality, we derive a sharp oracle inequality in the convex regression problem under an arbitrary dimension. We also illustrate the theory in Gaussian process regression and density estimation problems.},
	urldate = {2022-04-07},
	journal = {arXiv:1611.01125 [math, stat]},
	author = {Bhattacharya, Anirban and Pati, Debdeep and Yang, Yun},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01125},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/BNAHL88Y/Bhattacharya et al. - 2016 - Bayesian fractional posteriors.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/23UFW443/1611.html:text/html},
}

@article{friel_marginal_2008,
	title = {Marginal likelihood estimation via power posteriors},
	volume = {70},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00650.x},
	doi = {10.1111/j.1467-9868.2007.00650.x},
	language = {en},
	number = {3},
	urldate = {2022-04-07},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Friel, N. and Pettitt, A. N.},
	month = jul,
	year = {2008},
	pages = {589--607},
	file = {Friel and Pettitt - 2008 - Marginal likelihood estimation via power posterior.pdf:/Users/marvin/Zotero/storage/5KGM8E32/Friel and Pettitt - 2008 - Marginal likelihood estimation via power posterior.pdf:application/pdf},
}

@article{li_improved_2021,
	title = {Improved marginal likelihood estimation via power posteriors and importance sampling},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407621002736},
	doi = {10.1016/j.jeconom.2021.11.009},
	language = {en},
	urldate = {2022-04-07},
	journal = {Journal of Econometrics},
	author = {Li, Yong and Wang, Nianling and Yu, Jun},
	month = dec,
	year = {2021},
	pages = {S0304407621002736},
	file = {Li et al. - 2021 - Improved marginal likelihood estimation via power .pdf:/Users/marvin/Zotero/storage/836QG6GH/Li et al. - 2021 - Improved marginal likelihood estimation via power .pdf:application/pdf},
}

@article{lotfi_bayesian_2022,
	title = {Bayesian {Model} {Selection}, the {Marginal} {Likelihood}, and {Generalization}},
	url = {http://arxiv.org/abs/2202.11678},
	abstract = {How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.},
	urldate = {2022-04-12},
	journal = {arXiv:2202.11678 [cs, stat]},
	author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.11678},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/TJXLZJ4H/Lotfi et al. - 2022 - Bayesian Model Selection, the Marginal Likelihood,.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/ATRFZYN8/2202.html:text/html},
}

@article{fong_marginal_2019,
	title = {On the marginal likelihood and cross-validation},
	url = {http://arxiv.org/abs/1905.08737},
	abstract = {In Bayesian statistics, the marginal likelihood, also known as the evidence, is used to evaluate model fit as it quantifies the joint probability of the data under the prior. In contrast, non-Bayesian models are typically compared using cross-validation on held-out data, either through \$k\$-fold partitioning or leave-\$p\$-out subsampling. We show that the marginal likelihood is formally equivalent to exhaustive leave-\$p\$-out cross-validation averaged over all values of \$p\$ and all held-out test sets when using the log posterior predictive probability as the scoring rule. Moreover, the log posterior predictive is the only coherent scoring rule under data exchangeability. This offers new insight into the marginal likelihood and cross-validation and highlights the potential sensitivity of the marginal likelihood to the choice of the prior. We suggest an alternative approach using cumulative cross-validation following a preparatory training phase. Our work has connections to prequential analysis and intrinsic Bayes factors but is motivated through a different course.},
	urldate = {2022-04-12},
	journal = {arXiv:1905.08737 [stat]},
	author = {Fong, Edwin and Holmes, Chris},
	month = sep,
	year = {2019},
	note = {arXiv: 1905.08737},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/CIKATTWH/Fong and Holmes - 2019 - On the marginal likelihood and cross-validation.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/CPAM6FEA/1905.html:text/html},
}

@article{tolstikhin_wasserstein_2019,
	title = {Wasserstein {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1711.01558},
	abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
	urldate = {2022-04-12},
	journal = {arXiv:1711.01558 [cs, stat]},
	author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
	month = dec,
	year = {2019},
	note = {arXiv: 1711.01558},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/8HIN9AFF/Tolstikhin et al. - 2019 - Wasserstein Auto-Encoders.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/XS5P5QTF/1711.html:text/html},
}

@incollection{kadane_bayesian_1980,
	title = {Bayesian decision theory and the simplification of models},
	booktitle = {Evaluation of econometric models},
	publisher = {Elsevier},
	author = {Kadane, Joseph B and Dickey, James M},
	year = {1980},
	pages = {245--268},
	file = {Kadane and Dickey - 1980 - Bayesian decision theory and the simplification of.pdf:/Users/marvin/Zotero/storage/QU2AP84Q/Kadane and Dickey - 1980 - Bayesian decision theory and the simplification of.pdf:application/pdf},
}

@article{kadane_methods_2004,
	title = {Methods and criteria for model selection},
	volume = {99},
	number = {465},
	journal = {Journal of the American statistical Association},
	author = {Kadane, Joseph B and Lazar, Nicole A},
	year = {2004},
	note = {Publisher: Taylor \& Francis},
	pages = {279--290},
	file = {Kadane and Lazar - 2004 - Methods and criteria for model selection.pdf:/Users/marvin/Zotero/storage/4EGED8QH/Kadane and Lazar - 2004 - Methods and criteria for model selection.pdf:application/pdf},
}

@article{lindley_comments_1997,
	title = {Some comments on {Bayes} factors},
	volume = {61},
	issn = {03783758},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378375896001899},
	doi = {10.1016/S0378-3758(96)00189-9},
	language = {en},
	number = {1},
	urldate = {2022-04-13},
	journal = {Journal of Statistical Planning and Inference},
	author = {Lindley, Dennis V.},
	month = may,
	year = {1997},
	pages = {181--189},
	file = {Lindley - 1997 - Some comments on Bayes factors.pdf:/Users/marvin/Zotero/storage/H99K84LL/Lindley - 1997 - Some comments on Bayes factors.pdf:application/pdf},
}

@article{schwaferts_how_2021,
	title = {How to {Guide} {Decisions} with {Bayes} {Factors}},
	url = {http://arxiv.org/abs/2110.09981},
	abstract = {Some scientific research questions ask to guide decisions and others do not. By their nature frequentist hypothesis-tests yield a dichotomous test decision as result, rendering them rather inappropriate for latter types of research questions. Bayes factors, however, are argued to be both able to refrain from making decisions and to be employed in guiding decisions. This paper elaborates on how to use a Bayes factor for guiding a decision. In this regard, its embedding within the framework of Bayesian decision theory is delineated, in which a (hypothesis-based) loss function needs to be specified. Typically, such a specification is difficult for an applied scientist as relevant information might be scarce, vague, partial, and ambiguous. To tackle this issue, a robust, interval-valued specification of this loss function shall be allowed, such that the essential but partial information can be included into the analysis as is. Further, the restriction of the prior distributions to be proper distributions (which is necessary to calculate Bayes factors) can be alleviated if a decision is of interest. Both the resulting framework of hypothesis-based Bayesian decision theory with robust loss function and how to derive optimal decisions from already existing Bayes factors are depicted by user-friendly and straightforward step-by-step guides.},
	urldate = {2022-04-13},
	journal = {arXiv:2110.09981 [stat]},
	author = {Schwaferts, Patrick and Augustin, Thomas},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09981},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/FRSU4X6J/Schwaferts and Augustin - 2021 - How to Guide Decisions with Bayes Factors.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/7NXNED3Y/2110.html:text/html},
}

@article{birnbaum_foundations_1962,
	title = {On the {Foundations} of {Statistical} {Inference}},
	volume = {57},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1962.10480660},
	doi = {10.1080/01621459.1962.10480660},
	language = {en},
	number = {298},
	urldate = {2022-04-14},
	journal = {Journal of the American Statistical Association},
	author = {Birnbaum, Allan},
	month = jun,
	year = {1962},
	pages = {269--306},
	file = {Birnbaum - 1962 - On the Foundations of Statistical Inference.pdf:/Users/marvin/Zotero/storage/U9V4BSYQ/Birnbaum - 1962 - On the Foundations of Statistical Inference.pdf:application/pdf},
}

@article{bernardo_bayesian_2002,
	title = {Bayesian {Hypothesis} {Testing}: {A} {Reference} {Approach}},
	volume = {70},
	issn = {03067734},
	shorttitle = {Bayesian {Hypothesis} {Testing}},
	url = {https://www.jstor.org/stable/10.2307/1403862?origin=crossref},
	doi = {10.2307/1403862},
	number = {3},
	urldate = {2022-04-14},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Bernardo, Jose M. and Rueda, Raul},
	month = dec,
	year = {2002},
	pages = {351},
	file = {30e83dfc-ab27-4738-8ed9-ab657248c9b0.pdf:/Users/marvin/Zotero/storage/FVHMPVUL/30e83dfc-ab27-4738-8ed9-ab657248c9b0.pdf:application/pdf;Submitted Version:/Users/marvin/Zotero/storage/PZGBSQZ8/Bernardo and Rueda - 2002 - Bayesian Hypothesis Testing A Reference Approach.pdf:application/pdf},
}

@article{liu_bayes_2008,
	title = {Bayes factors: {Prior} sensitivity and model generalizability},
	volume = {52},
	issn = {00222496},
	shorttitle = {Bayes factors},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002224960800028X},
	doi = {10.1016/j.jmp.2008.03.002},
	language = {en},
	number = {6},
	urldate = {2022-04-14},
	journal = {Journal of Mathematical Psychology},
	author = {Liu, Charles C. and Aitkin, Murray},
	month = dec,
	year = {2008},
	pages = {362--375},
	file = {Liu and Aitkin - 2008 - Bayes factors Prior sensitivity and model general.pdf:/Users/marvin/Zotero/storage/NE45KE2Z/Liu and Aitkin - 2008 - Bayes factors Prior sensitivity and model general.pdf:application/pdf},
}

@book{lee_introduction_2000,
	address = {New York},
	title = {Introduction to topological manifolds},
	isbn = {9786610010608 9780387987590 9780387227276},
	language = {English},
	publisher = {Springer},
	author = {Lee, John M},
	year = {2000},
	note = {OCLC: 1295480606},
	file = {Lee and MyiLibrary - 2000 - Introduction to topological manifolds.pdf:/Users/marvin/Zotero/storage/GUJ8RUR9/Lee and MyiLibrary - 2000 - Introduction to topological manifolds.pdf:application/pdf},
}

@article{im_gplom_2013,
	title = {{GPLOM}: {The} {Generalized} {Plot} {Matrix} for {Visualizing} {Multidimensional} {Multivariate} {Data}},
	volume = {19},
	issn = {1077-2626},
	shorttitle = {{GPLOM}},
	url = {http://ieeexplore.ieee.org/document/6634192/},
	doi = {10.1109/TVCG.2013.160},
	number = {12},
	urldate = {2022-05-11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Im, Jean-Francois and McGuffin, Michael J. and Leung, Rock},
	month = dec,
	year = {2013},
	pages = {2606--2614},
	file = {Im et al. - 2013 - GPLOM The Generalized Plot Matrix for Visualizing.pdf:/Users/marvin/Zotero/storage/RF4A9K6B/Im et al. - 2013 - GPLOM The Generalized Plot Matrix for Visualizing.pdf:application/pdf},
}

@article{gemmell_population_2014,
	title = {Population of {Computational} {Rabbit}-{Specific} {Ventricular} {Action} {Potential} {Models} for {Investigating} {Sources} of {Variability} in {Cellular} {Repolarisation}},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0090112},
	doi = {10.1371/journal.pone.0090112},
	language = {en},
	number = {2},
	urldate = {2022-05-11},
	journal = {PLoS ONE},
	author = {Gemmell, Philip and Burrage, Kevin and Rodriguez, Blanca and Quinn, T. Alexander},
	editor = {Bondarenko, Vladimir E.},
	month = feb,
	year = {2014},
	keywords = {Dimensional Stacking},
	pages = {e90112},
	file = {Full Text:/Users/marvin/Zotero/storage/MD5UR9Q9/Gemmell et al. - 2014 - Population of Computational Rabbit-Specific Ventri.pdf:application/pdf},
}

@article{snider_neural_2011,
	title = {A neural circuit for angular velocity computation},
	issn = {16625110},
	url = {http://journal.frontiersin.org/article/10.3389/fncir.2010.00123/abstract},
	doi = {10.3389/fncir.2010.00123},
	urldate = {2022-05-11},
	journal = {Frontiers in Neural Circuits},
	author = {{Snider}},
	year = {2011},
	keywords = {Dimensional Stacking},
	file = {Full Text:/Users/marvin/Zotero/storage/MJVAGKR6/Snider - 2011 - A neural circuit for angular velocity computation.pdf:application/pdf},
}

@article{griewank_high_2018,
	title = {High dimensional integration of kinks and jumps—{Smoothing} by preintegration},
	volume = {344},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042718301778},
	doi = {10.1016/j.cam.2018.04.009},
	language = {en},
	urldate = {2022-05-11},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Griewank, Andreas and Kuo, Frances Y. and Leövey, Hernan and Sloan, Ian H.},
	month = dec,
	year = {2018},
	keywords = {Pre-Integration},
	pages = {259--274},
	file = {Submitted Version:/Users/marvin/Zotero/storage/IY89EG7N/Griewank et al. - 2018 - High dimensional integration of kinks and jumps—Sm.pdf:application/pdf},
}

@article{liu_pre-integration_2022,
	title = {Pre-integration via {Active} {Subspaces}},
	url = {http://arxiv.org/abs/2202.02682},
	abstract = {Pre-integration is an extension of conditional Monte Carlo to quasi-Monte Carlo and randomized quasi-Monte Carlo. It can reduce but not increase the variance in Monte Carlo. For quasi-Monte Carlo it can bring about improved regularity of the integrand with potentially greatly improved accuracy. Pre-integration is ordinarily done by integrating out one of \$d\$ input variables to a function. In the common case of a Gaussian integral one can also pre-integrate over any linear combination of variables. We propose to do that and we choose the first eigenvector in an active subspace decomposition to be the pre-integrated linear combination. We find in numerical examples that this active subspace pre-integration strategy is competitive with pre-integrating the first variable in the principal components construction on the Asian option where principal components are known to be very effective. It outperforms other pre-integration methods on some basket options where there is no well established default. We show theoretically that, just as in Monte Carlo, pre-integration can reduce but not increase the variance when one uses scrambled net integration. We show that the lead eigenvector in an active subspace decomposition is closely related to the vector that maximizes a less computationally tractable criterion using a Sobol' index to find the most important linear combination of Gaussian variables. They optimize similar expectations involving the gradient. We show that the Sobol' index criterion for the leading eigenvector is invariant to the way that one chooses the remaining \$d-1\$ eigenvectors with which to sample the Gaussian vector.},
	urldate = {2022-05-11},
	journal = {arXiv:2202.02682 [cs, math, stat]},
	author = {Liu, Sifan and Owen, Art B.},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.02682},
	keywords = {Mathematics - Statistics Theory, Pre-Integration, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/VDLCVIUM/Liu and Owen - 2022 - Pre-integration via Active Subspaces.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/W8KW7I5F/2202.html:text/html},
}

@article{bachthaler_continuous_2008,
	title = {Continuous {Scatterplots}},
	volume = {14},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/4658159/},
	doi = {10.1109/TVCG.2008.119},
	number = {6},
	urldate = {2022-05-11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bachthaler, S. and Weiskopf, D.},
	month = nov,
	year = {2008},
	keywords = {Continuous Scatterplots},
	pages = {1428--1435},
	file = {Bachthaler and Weiskopf - 2008 - Continuous Scatterplots.pdf:/Users/marvin/Zotero/storage/UGCE2RKN/Bachthaler and Weiskopf - 2008 - Continuous Scatterplots.pdf:application/pdf},
}

@article{sekse_kristiansen_visception_2020,
	title = {Visception: {An} interactive visual framework for nested visualization design},
	volume = {92},
	issn = {00978493},
	shorttitle = {Visception},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0097849320301254},
	doi = {10.1016/j.cag.2020.08.007},
	language = {en},
	urldate = {2022-05-11},
	journal = {Computers \& Graphics},
	author = {Sekse Kristiansen, Yngve and Bruckner, Stefan},
	month = nov,
	year = {2020},
	pages = {13--27},
	file = {Full Text:/Users/marvin/Zotero/storage/DY2FBKQE/Sekse Kristiansen and Bruckner - 2020 - Visception An interactive visual framework for ne.pdf:application/pdf;Sekse Kristiansen and Bruckner - 2020 - Visception An interactive visual framework for ne.pdf:/Users/marvin/Zotero/storage/F452A3CK/Sekse Kristiansen and Bruckner - 2020 - Visception An interactive visual framework for ne.pdf:application/pdf},
}

@article{liu_visualizing_2017,
	title = {Visualizing {High}-{Dimensional} {Data}: {Advances} in the {Past} {Decade}},
	volume = {23},
	issn = {1077-2626},
	shorttitle = {Visualizing {High}-{Dimensional} {Data}},
	url = {http://ieeexplore.ieee.org/document/7784854/},
	doi = {10.1109/TVCG.2016.2640960},
	number = {3},
	urldate = {2022-05-11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Shusen and Maljovec, Dan and Wang, Bei and Bremer, Peer-Timo and Pascucci, Valerio},
	month = mar,
	year = {2017},
	pages = {1249--1268},
	file = {Liu et al. - 2017 - Visualizing High-Dimensional Data Advances in the.pdf:/Users/marvin/Zotero/storage/SWQQEPCQ/Liu et al. - 2017 - Visualizing High-Dimensional Data Advances in the.pdf:application/pdf;Submitted Version:/Users/marvin/Zotero/storage/STXKXPWC/Liu et al. - 2017 - Visualizing High-Dimensional Data Advances in the.pdf:application/pdf},
}

@article{oladyshkin_connection_2019,
	title = {The {Connection} between {Bayesian} {Inference} and {Information} {Theory} for {Model} {Selection}, {Information} {Gain} and {Experimental} {Design}},
	volume = {21},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/21/11/1081},
	doi = {10.3390/e21111081},
	abstract = {We show a link between Bayesian inference and information theory that is useful for model selection, assessment of information entropy and experimental design. We align Bayesian model evidence (BME) with relative entropy and cross entropy in order to simplify computations using prior-based (Monte Carlo) or posterior-based (Markov chain Monte Carlo) BME estimates. On the one hand, we demonstrate how Bayesian model selection can profit from information theory to estimate BME values via posterior-based techniques. Hence, we use various assumptions including relations to several information criteria. On the other hand, we demonstrate how relative entropy can profit from BME to assess information entropy during Bayesian updating and to assess utility in Bayesian experimental design. Specifically, we emphasize that relative entropy can be computed avoiding unnecessary multidimensional integration from both prior and posterior-based sampling techniques. Prior-based computation does not require any assumptions, however posterior-based estimates require at least one assumption. We illustrate the performance of the discussed estimates of BME, information entropy and experiment utility using a transparent, non-linear example. The multivariate Gaussian posterior estimate includes least assumptions and shows the best performance for BME estimation, information entropy and experiment utility from posterior-based sampling.},
	language = {en},
	number = {11},
	urldate = {2022-05-11},
	journal = {Entropy},
	author = {Oladyshkin, Sergey and Nowak, Wolfgang},
	month = nov,
	year = {2019},
	pages = {1081},
	file = {Full Text:/Users/marvin/Zotero/storage/KMINSW5P/Oladyshkin and Nowak - 2019 - The Connection between Bayesian Inference and Info.pdf:application/pdf},
}

@book{kotz_continuous_2000,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Continuous multivariate distributions},
	isbn = {978-0-471-18387-7},
	publisher = {Wiley},
	author = {Kotz, Samuel and Johnson, Norman Lloyd and Balakrishnan, N. and Johnson, Norman Lloyd},
	year = {2000},
	keywords = {Distribution (Probability theory), Multivariate analysis},
	file = {Kotz et al. - 2000 - Continuous multivariate distributions.pdf:/Users/marvin/Zotero/storage/FXWFM7VA/Kotz et al. - 2000 - Continuous multivariate distributions.pdf:application/pdf},
}

@article{aitchison_logistic-normal_1980,
	title = {Logistic-{Normal} {Distributions}: {Some} {Properties} and {Uses}},
	volume = {67},
	issn = {00063444},
	shorttitle = {Logistic-{Normal} {Distributions}},
	url = {https://www.jstor.org/stable/2335470?origin=crossref},
	doi = {10.2307/2335470},
	number = {2},
	urldate = {2022-05-13},
	journal = {Biometrika},
	author = {Aitchison, J. and Shen, S. M.},
	month = aug,
	year = {1980},
	pages = {261},
	file = {Aitchison and Shen - 1980 - Logistic-Normal Distributions Some Properties and.pdf:/Users/marvin/Zotero/storage/2IS9Q9LS/Aitchison and Shen - 1980 - Logistic-Normal Distributions Some Properties and.pdf:application/pdf},
}

@article{dong_interactive_2020,
	title = {An interactive web-based dashboard to track {COVID}-19 in real time},
	volume = {20},
	issn = {14733099},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1473309920301201},
	doi = {10.1016/S1473-3099(20)30120-1},
	language = {en},
	number = {5},
	urldate = {2022-05-18},
	journal = {The Lancet Infectious Diseases},
	author = {Dong, Ensheng and Du, Hongru and Gardner, Lauren},
	month = may,
	year = {2020},
	pages = {533--534},
	file = {Full Text:/Users/marvin/Zotero/storage/FJ7MH7H2/Dong et al. - 2020 - An interactive web-based dashboard to track COVID-.pdf:application/pdf},
}

@article{zheng_uncertainty_2021,
	title = {Uncertainty in {Continuous} {Scatterplots}, {Continuous} {Parallel} {Coordinates}, and {Fibers}},
	volume = {27},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/9222253/},
	doi = {10.1109/TVCG.2020.3030466},
	number = {2},
	urldate = {2022-05-19},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zheng, Boyan and Sadlo, Filip},
	month = feb,
	year = {2021},
	pages = {1819--1828},
	file = {Zheng and Sadlo - 2021 - Uncertainty in Continuous Scatterplots, Continuous.pdf:/Users/marvin/Zotero/storage/QD622P27/Zheng and Sadlo - 2021 - Uncertainty in Continuous Scatterplots, Continuous.pdf:application/pdf},
}

@book{henry_purrr_2020,
	title = {purrr: {Functional} {Programming} {Tools}},
	url = {https://CRAN.R-project.org/package=purrr},
	author = {Henry, Lionel and Wickham, Hadley},
	year = {2020},
}

@book{murdoch_rgl_2021,
	title = {rgl: {3D} {Visualization} {Using} {OpenGL}},
	url = {https://CRAN.R-project.org/package=rgl},
	author = {Murdoch, Duncan and Adler, Daniel},
	year = {2021},
}

@book{woo_opengl_1999,
	title = {{OpenGL} programming guide: the official guide to learning {OpenGL}, version 1.2},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Woo, Mason and Neider, Jackie and Davis, Tom and Shreiner, Dave},
	year = {1999},
}

@incollection{schlager_morpho_2017,
	title = {Morpho and {Rvcg} – {Shape} {Analysis} in {R}},
	isbn = {978-0-12-810493-4},
	booktitle = {Statistical {Shape} and {Deformation} {Analysis}},
	publisher = {Academic Press},
	author = {Schlager, Stefan},
	editor = {Zheng, Guoyan and Li, Shuo and Szekely, Gabor},
	year = {2017},
	pages = {217--256},
}

@article{burkner_advanced_2018,
	title = {Advanced {Bayesian} {Multilevel} {Modeling} with the {R} {Package} brms},
	volume = {10},
	doi = {10.32614/RJ-2018-017},
	number = {1},
	journal = {The R Journal},
	author = {Bürkner, Paul-Christian},
	year = {2018},
	pages = {395--411},
}

@article{burkner_bayesian_2021,
	title = {Bayesian {Item} {Response} {Modeling} in {R} with brms and {Stan}},
	volume = {100},
	doi = {10.18637/jss.v100.i05},
	number = {5},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2021},
	pages = {1--54},
}

@book{wickham_ggplot2_2016,
	title = {ggplot2: {Elegant} {Graphics} for {Data} {Analysis}},
	isbn = {978-3-319-24277-4},
	url = {https://ggplot2.tidyverse.org},
	publisher = {Springer-Verlag New York},
	author = {Wickham, Hadley},
	year = {2016},
}

@book{r_core_team_r_2021,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2021},
}

@misc{stan_development_team_rstan_2020,
	title = {{RStan}: the {R} interface to {Stan}},
	url = {https://mc-stan.org/},
	author = {{Stan Development Team}},
	year = {2020},
}

@misc{stan_development_team_stan_2022,
	title = {Stan {Modeling} {Language} {Users} {Guide} and {Reference} {Manual}},
	author = {{Stan Development Team}},
	year = {2022},
	note = {https://mc-stan.org},
}

@article{piironen_projective_2020,
	title = {Projective inference in high-dimensional problems: {Prediction} and feature selection},
	volume = {14},
	issn = {1935-7524},
	shorttitle = {Projective inference in high-dimensional problems},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-14/issue-1/Projective-inference-in-high-dimensional-problems--Prediction-and-feature/10.1214/20-EJS1711.full},
	doi = {10.1214/20-EJS1711},
	number = {1},
	urldate = {2022-06-27},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
	month = jan,
	year = {2020},
	file = {Full Text:/Users/marvin/Zotero/storage/ZKNAYSNH/Piironen et al. - 2020 - Projective inference in high-dimensional problems.pdf:application/pdf},
}

@inproceedings{leblanc_exploring_1990,
	address = {San Francisco, CA, USA},
	title = {Exploring {N}-dimensional databases},
	isbn = {978-0-8186-2083-6},
	url = {http://ieeexplore.ieee.org/document/146386/},
	doi = {10.1109/VISUAL.1990.146386},
	urldate = {2022-06-30},
	booktitle = {Proceedings of the {First} {IEEE} {Conference} on {Visualization}: {Visualization} `90},
	publisher = {IEEE Comput. Soc. Press},
	author = {LeBlanc, J. and Ward, M.O. and Wittels, N.},
	year = {1990},
	pages = {230--237},
	file = {LeBlanc et al. - 1990 - Exploring N-dimensional databases.pdf:/Users/marvin/Zotero/storage/UVCELR9U/LeBlanc et al. - 1990 - Exploring N-dimensional databases.pdf:application/pdf},
}

@inproceedings{mihalisin_visualization_1991,
	address = {San Diego, CA, USA},
	title = {Visualization and analysis of multi-variate data: a technique for all fields},
	isbn = {978-0-8186-2245-8},
	shorttitle = {Visualization and analysis of multi-variate data},
	url = {http://ieeexplore.ieee.org/document/175796/},
	doi = {10.1109/VISUAL.1991.175796},
	urldate = {2022-06-30},
	booktitle = {Proceeding {Visualization} '91},
	publisher = {IEEE Comput. Soc. Press},
	author = {Mihalisin, T. and Timlin, J. and Schwegler, J.},
	year = {1991},
	pages = {171--178,},
}

@article{weiskopf_uncertainty_2022,
	title = {Uncertainty {Visualization}: {Concepts}, {Methods}, and {Applications} in {Biological} {Data} {Visualization}},
	volume = {2},
	issn = {2673-7647},
	shorttitle = {Uncertainty {Visualization}},
	url = {https://www.frontiersin.org/articles/10.3389/fbinf.2022.793819/full},
	doi = {10.3389/fbinf.2022.793819},
	abstract = {This paper provides an overview of uncertainty visualization in general, along with specific examples of applications in bioinformatics. Starting from a processing and interaction pipeline of visualization, components are discussed that are relevant for handling and visualizing uncertainty introduced with the original data and at later stages in the pipeline, which shows the importance of making the stages of the pipeline aware of uncertainty and allowing them to propagate uncertainty. We detail concepts and methods for visual mappings of uncertainty, distinguishing between explicit and implict representations of distributions, different ways to show summary statistics, and combined or hybrid visualizations. The basic concepts are illustrated for several examples of graph visualization under uncertainty. Finally, this review paper discusses implications for the visualization of biological data and future research directions.},
	urldate = {2022-07-01},
	journal = {Frontiers in Bioinformatics},
	author = {Weiskopf, Daniel},
	month = feb,
	year = {2022},
	pages = {793819},
	file = {Full Text:/Users/marvin/Zotero/storage/LZPM78AY/Weiskopf - 2022 - Uncertainty Visualization Concepts, Methods, and .pdf:application/pdf},
}

@incollection{dienstfrey_quantification_2012,
	address = {Berlin, Heidelberg},
	title = {From {Quantification} to {Visualization}: {A} {Taxonomy} of {Uncertainty} {Visualization} {Approaches}},
	volume = {377},
	isbn = {978-3-642-32676-9 978-3-642-32677-6},
	shorttitle = {From {Quantification} to {Visualization}},
	url = {http://link.springer.com/10.1007/978-3-642-32677-6_15},
	urldate = {2022-07-01},
	booktitle = {Uncertainty {Quantification} in {Scientific} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Potter, Kristin and Rosen, Paul and Johnson, Chris R.},
	editor = {Dienstfrey, Andrew M. and Boisvert, Ronald F.},
	year = {2012},
	doi = {10.1007/978-3-642-32677-6_15},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {226--249},
	file = {Full Text:/Users/marvin/Zotero/storage/JVEGN4N3/Potter et al. - 2012 - From Quantification to Visualization A Taxonomy o.pdf:application/pdf},
}

@incollection{hutchison_summarizing_2010,
	address = {Berlin, Heidelberg},
	title = {Summarizing and {Visualizing} {Uncertainty} in {Non}-rigid {Registration}},
	volume = {6362},
	isbn = {978-3-642-15744-8 978-3-642-15745-5},
	url = {http://link.springer.com/10.1007/978-3-642-15745-5_68},
	language = {en},
	urldate = {2022-07-01},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Risholm, Petter and Pieper, Steve and Samset, Eigil and Wells, William M.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Jiang, Tianzi and Navab, Nassir and Pluim, Josien P. W. and Viergever, Max A.},
	year = {2010},
	doi = {10.1007/978-3-642-15745-5_68},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {554--561},
	file = {Full Text:/Users/marvin/Zotero/storage/DLZMD746/Risholm et al. - 2010 - Summarizing and Visualizing Uncertainty in Non-rig.pdf:application/pdf},
}

@incollection{hansen_overview_2014,
	address = {London},
	title = {Overview and {State}-of-the-{Art} of {Uncertainty} {Visualization}},
	isbn = {978-1-4471-6496-8 978-1-4471-6497-5},
	url = {http://link.springer.com/10.1007/978-1-4471-6497-5_1},
	urldate = {2022-08-02},
	booktitle = {Scientific {Visualization}},
	publisher = {Springer London},
	author = {Bonneau, Georges-Pierre and Hege, Hans-Christian and Johnson, Chris R. and Oliveira, Manuel M. and Potter, Kristin and Rheingans, Penny and Schultz, Thomas},
	editor = {Hansen, Charles D. and Chen, Min and Johnson, Christopher R. and Kaufman, Arie E. and Hagen, Hans},
	year = {2014},
	doi = {10.1007/978-1-4471-6497-5_1},
	note = {Series Title: Mathematics and Visualization},
	pages = {3--27},
}

@incollection{balakrishnan_uncertainty_2021,
	edition = {1},
	title = {Uncertainty {Visualization}},
	isbn = {978-1-118-44511-2},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118445112.stat08296},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {Wiley {StatsRef}: {Statistics} {Reference} {Online}},
	publisher = {Wiley},
	author = {Padilla, Lace and Kay, Matthew and Hullman, Jessica},
	editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
	month = feb,
	year = {2021},
	doi = {10.1002/9781118445112.stat08296},
	pages = {1--18},
	file = {Submitted Version:/Users/marvin/Zotero/storage/QJXM3WI7/Padilla et al. - 2021 - Uncertainty Visualization.pdf:application/pdf},
}

@book{salomon_concise_2008,
	address = {London},
	series = {Undergraduate topics in computer science},
	title = {A concise introduction to data compression},
	isbn = {978-1-84800-072-8},
	abstract = {This clearly written book offers readers a succinct foundation to the most important topics in the field of data compression. Part I presents the basic approaches to data compression and describes a few popular techniques and methods that are commonly used to compress data. The reader will discover essential concepts. Part II concentrates on advanced techniques, such as arithmetic coding, orthogonal transforms, subband transforms and Burrows-Wheeler transform. This book is the perfect reference for advanced undergraduates in computer science and requires a minimum of mathematics. An author-maintained website provides errata and auxiliary material},
	language = {eng},
	publisher = {Springer},
	author = {Salomon, David},
	year = {2008},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	issn = {15662535},
	shorttitle = {A review of uncertainty quantification in deep learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521001081},
	doi = {10.1016/j.inffus.2021.05.008},
	language = {en},
	urldate = {2022-08-02},
	journal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = dec,
	year = {2021},
	pages = {243--297},
	file = {Full Text:/Users/marvin/Zotero/storage/9A54MG7S/Abdar et al. - 2021 - A review of uncertainty quantification in deep lea.pdf:application/pdf},
}

@book{sullivan_introduction_2015,
	address = {Cham},
	edition = {1st ed. 2015},
	series = {Texts in {Applied} {Mathematics}},
	title = {Introduction to {Uncertainty} {Quantification}},
	isbn = {978-3-319-23395-6},
	abstract = {Uncertainty quantification is a topic of increasing practical importance at the intersection of applied mathematics, statistics, computation, and numerous application areas in science and engineering. This text provides a framework in which the main objectives of the field of uncertainty quantification are defined, and an overview of the range of mathematical methods by which they can be achieved. Complete with exercises throughout, the book will equip readers with both theoretical understanding and practical experience of the key mathematical and algorithmic tools underlying the treatment of uncertainty in modern applied mathematics. Students and readers alike are encouraged to apply the mathematical methods discussed in this book to their own favourite problems to understand their strengths and weaknesses, also making the text suitable as a self-study. This text is designed as an introduction to uncertainty quantification for senior undergraduate and graduate students with a mathematical or statistical background, and also for researchers from the mathematical sciences or from applications areas who are interested in the field. T. J. Sullivan was Warwick Zeeman Lecturer at the Mathematics Institute of the University of Warwick, United Kingdom, from 2012 to 2015. Since 2015, he is Junior Professor of Applied Mathematics at the Free University of Berlin, Germany, with specialism in Uncertainty and Risk Quantification},
	number = {63},
	publisher = {Springer International Publishing : Imprint: Springer},
	author = {Sullivan, T. J.},
	year = {2015},
	doi = {10.1007/978-3-319-23395-6},
	keywords = {Applied mathematics, Engineering mathematics, Mathematical and Computational Engineering, Mathematical optimization, Mathematical physics, Numerical analysis, Numerical Analysis, Optimization, Probabilities, Probability Theory and Stochastic Processes, Theoretical, Mathematical and Computational Physics},
}

@misc{psaros_uncertainty_2022,
	title = {Uncertainty {Quantification} in {Scientific} {Machine} {Learning}: {Methods}, {Metrics}, and {Comparisons}},
	shorttitle = {Uncertainty {Quantification} in {Scientific} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2201.07766},
	abstract = {Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to NN hyperparameters, overparametrization, optimization and sampling errors as well as model misspecification. Although there are some recent works on uncertainty quantification (UQ) in NNs, there is no systematic investigation of suitable methods towards quantifying the total uncertainty effectively and efficiently even for function approximation, and there is even less work on solving partial differential equations and learning operator mappings between infinite-dimensional function spaces using NNs. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the UQ methods employed, which we will make available as open-source library of all codes included in this framework.},
	urldate = {2022-08-02},
	publisher = {arXiv},
	author = {Psaros, Apostolos F. and Meng, Xuhui and Zou, Zongren and Guo, Ling and Karniadakis, George Em},
	month = jan,
	year = {2022},
	note = {arXiv:2201.07766 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/N2G9F4HI/Psaros et al. - 2022 - Uncertainty Quantification in Scientific Machine L.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/KIUV5AS8/2201.html:text/html},
}

@article{volodina_importance_2021,
	title = {The importance of uncertainty quantification in model reproducibility},
	volume = {379},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0071},
	doi = {10.1098/rsta.2020.0071},
	abstract = {Many computer models possess high-dimensional input spaces and substantial computational time to produce a single model evaluation. Although such models are often ‘deterministic’, these models suffer from a wide range of uncertainties. We argue that uncertainty quantification is crucial for computer model validation and reproducibility. We present a statistical framework, termed history matching, for performing global parameter search by comparing model output to the observed data. We employ Gaussian process (GP) emulators to produce fast predictions about model behaviour at the arbitrary input parameter settings allowing output uncertainty distributions to be calculated. History matching identifies sets of input parameters that give rise to acceptable matches between observed data and model output given our representation of uncertainties. Modellers could proceed by simulating computer models’ outputs of interest at these identified parameter settings and producing a range of predictions. The variability in model results is crucial for inter-model comparison as well as model development. We illustrate the performance of emulation and history matching on a simple one-dimensional toy model and in application to a climate model.
            
              This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification
              in silico
              ’.},
	language = {en},
	number = {2197},
	urldate = {2022-08-02},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Volodina, Victoria and Challenor, Peter},
	month = may,
	year = {2021},
	pages = {rsta.2020.0071, 20200071},
	file = {Full Text:/Users/marvin/Zotero/storage/UMWJA6AJ/Volodina and Challenor - 2021 - The importance of uncertainty quantification in mo.pdf:application/pdf},
}

@book{konishi_information_2008,
	address = {New York},
	series = {Springer series in statistics},
	title = {Information criteria and statistical modeling},
	isbn = {978-0-387-71886-6},
	publisher = {Springer},
	author = {Konishi, Sadanori and Kitagawa, G.},
	year = {2008},
	keywords = {Information modeling, Mathematical analysis, Stochastic analysis},
}

@article{konishi_generalised_1996,
	title = {Generalised information criteria in model selection},
	volume = {83},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/83.4.875},
	doi = {10.1093/biomet/83.4.875},
	language = {en},
	number = {4},
	urldate = {2022-08-02},
	journal = {Biometrika},
	author = {Konishi, S},
	month = dec,
	year = {1996},
	pages = {875--890},
}

@article{akaike_new_1974,
	title = {A new look at the statistical model identification},
	volume = {19},
	issn = {0018-9286},
	url = {http://ieeexplore.ieee.org/document/1100705/},
	doi = {10.1109/TAC.1974.1100705},
	language = {en},
	number = {6},
	urldate = {2022-08-02},
	journal = {IEEE Transactions on Automatic Control},
	author = {Akaike, H.},
	month = dec,
	year = {1974},
	pages = {716--723},
	file = {Accepted Version:/Users/marvin/Zotero/storage/FZRH8EGZ/Akaike - 1974 - A new look at the statistical model identification.pdf:application/pdf},
}

@article{rissanen_modeling_1978,
	title = {Modeling by shortest data description},
	volume = {14},
	issn = {00051098},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0005109878900055},
	doi = {10.1016/0005-1098(78)90005-5},
	language = {en},
	number = {5},
	urldate = {2022-08-02},
	journal = {Automatica},
	author = {Rissanen, J.},
	month = sep,
	year = {1978},
	pages = {465--471},
}

@misc{grunwald_tutorial_2004,
	title = {A tutorial introduction to the minimum description length principle},
	url = {http://arxiv.org/abs/math/0406077},
	abstract = {This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection "Advances in Minimum Description Length: Theory and Application" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).},
	urldate = {2022-08-02},
	publisher = {arXiv},
	author = {Grunwald, Peter},
	month = jun,
	year = {2004},
	note = {arXiv:math/0406077},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, 6201, 6801, 68T05, 68T10, 9401, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/63JSVM4V/Grunwald - 2004 - A tutorial introduction to the minimum description.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/8USYIAPA/0406077.html:text/html},
}

@article{vehtari_bayesian_2002,
	title = {Bayesian {Model} {Assessment} and {Comparison} {Using} {Cross}-{Validation} {Predictive} {Densities}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/14/10/2439-2468/6640},
	doi = {10.1162/08997660260293292},
	abstract = {In this work, we discuss practical methods for the assessment, comparison, and selection of complex hierarchical Bayesian models. A natural way to assess the goodness of the model is to estimate its future predictive capability by estimating expected utilities. Instead of just making a point estimate, it is important to obtain the distribution of the expected utility estimate because it describes the uncertainty in the estimate. The distributions of the expected utility estimates can also be used to compare models, for example, by computing the probability of one model having a better expected utility than some other model. We propose an approach using cross-validation predictive densities to obtain expected utility estimates and Bayesian bootstrap to obtain samples from their distributions. We also discuss the probabilistic assumptions made and properties of two practical cross-validation methods, importance sampling and k-fold cross-validation. As illustrative examples, we use multilayer perceptron neural networks and gaussian processes with Markov chain Monte Carlo sampling in one toy problem and two challenging real-world problems.},
	language = {en},
	number = {10},
	urldate = {2022-08-02},
	journal = {Neural Computation},
	author = {Vehtari, Aki and Lampinen, Jouko},
	month = oct,
	year = {2002},
	pages = {2439--2468},
}

@article{vehtari_survey_2012,
	title = {A survey of {Bayesian} predictive methods for model assessment, selection and comparison},
	volume = {6},
	issn = {1935-7516},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-6/issue-none/A-survey-of-Bayesian-predictive-methods-for-model-assessment-selection/10.1214/12-SS102.full},
	doi = {10.1214/12-SS102},
	number = {none},
	urldate = {2022-08-02},
	journal = {Statistics Surveys},
	author = {Vehtari, Aki and Ojanen, Janne},
	month = jan,
	year = {2012},
	file = {Full Text:/Users/marvin/Zotero/storage/DKWZNBYG/Vehtari and Ojanen - 2012 - A survey of Bayesian predictive methods for model .pdf:application/pdf},
}

@misc{vehtari_pareto_2021,
	title = {Pareto {Smoothed} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1507.02646},
	abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
	urldate = {2022-08-02},
	publisher = {arXiv},
	author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
	month = feb,
	year = {2021},
	note = {arXiv:1507.02646 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/JWAHVQ4Y/Vehtari et al. - 2021 - Pareto Smoothed Importance Sampling.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/QEITEJ7P/1507.html:text/html},
}

@article{burkner_approximate_2020,
	title = {Approximate leave-future-out cross-validation for {Bayesian} time series models},
	volume = {90},
	issn = {0094-9655, 1563-5163},
	url = {http://arxiv.org/abs/1902.06281},
	doi = {10.1080/00949655.2020.1783262},
	abstract = {One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. Exact cross-validation for Bayesian models is often computationally expensive, but approximate cross-validation methods have been developed, most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate because the information from future observations is available to influence predictions of the past. To properly account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation.},
	number = {14},
	urldate = {2022-08-02},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
	month = sep,
	year = {2020},
	note = {arXiv:1902.06281 [stat]},
	keywords = {Statistics - Methodology},
	pages = {2499--2523},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/GW9S6CW9/Bürkner et al. - 2020 - Approximate leave-future-out cross-validation for .pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/FXXMWICC/1902.html:text/html},
}

@article{van_dongen_prior_2006,
	title = {Prior specification in {Bayesian} statistics: {Three} cautionary tales},
	volume = {242},
	issn = {00225193},
	shorttitle = {Prior specification in {Bayesian} statistics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022519306000609},
	doi = {10.1016/j.jtbi.2006.02.002},
	language = {en},
	number = {1},
	urldate = {2022-08-02},
	journal = {Journal of Theoretical Biology},
	author = {Van Dongen, Stefan},
	month = sep,
	year = {2006},
	pages = {90--100},
}

@article{zitzmann_prior_2021,
	title = {Prior {Specification} for {More} {Stable} {Bayesian} {Estimation} of {Multilevel} {Latent} {Variable} {Models} in {Small} {Samples}: {A} {Comparative} {Investigation} of {Two} {Different} {Approaches}},
	volume = {11},
	issn = {1664-1078},
	shorttitle = {Prior {Specification} for {More} {Stable} {Bayesian} {Estimation} of {Multilevel} {Latent} {Variable} {Models} in {Small} {Samples}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2020.611267/full},
	doi = {10.3389/fpsyg.2020.611267},
	abstract = {Bayesian approaches for estimating multilevel latent variable models can be beneficial in small samples. Prior distributions can be used to overcome small sample problems, for example, when priors that increase the accuracy of estimation are chosen. This article discusses two different but not mutually exclusive approaches for specifying priors. Both approaches aim at stabilizing estimators in such a way that the Mean Squared Error (MSE) of the estimator of the between-group slope will be small. In the first approach, the MSE is decreased by specifying a slightly informative prior for the group-level variance of the predictor variable, whereas in the second approach, the decrease is achieved directly by using a slightly informative prior for the slope. Mathematical and graphical inspections suggest that both approaches can be effective for reducing the MSE in small samples, thus rendering them attractive in these situations. The article also discusses how these approaches can be implemented in M
              plus
              .},
	urldate = {2022-08-02},
	journal = {Frontiers in Psychology},
	author = {Zitzmann, Steffen and Helm, Christoph and Hecht, Martin},
	month = jan,
	year = {2021},
	pages = {611267},
	file = {Full Text:/Users/marvin/Zotero/storage/AQGKEIWJ/Zitzmann et al. - 2021 - Prior Specification for More Stable Bayesian Estim.pdf:application/pdf},
}

@article{dellaportas_joint_2012,
	title = {Joint {Specification} of {Model} {Space} and {Parameter} {Space} {Prior} {Distributions}},
	volume = {27},
	issn = {0883-4237},
	url = {http://arxiv.org/abs/1207.5651},
	doi = {10.1214/11-STS369},
	abstract = {We consider the specification of prior distributions for Bayesian model comparison, focusing on regression-type models. We propose a particular joint specification of the prior distribution across models so that sensitivity of posterior model probabilities to the dispersion of prior distributions for the parameters of individual models (Lindley's paradox) is diminished. We illustrate the behavior of inferential and predictive posterior quantities in linear and log-linear regressions under our proposed prior densities with a series of simulated and real data examples.},
	number = {2},
	urldate = {2022-08-02},
	journal = {Statistical Science},
	author = {Dellaportas, Petros and Forster, Jonathan J. and Ntzoufras, Ioannis},
	month = may,
	year = {2012},
	note = {arXiv:1207.5651 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/L5A5NBQJ/Dellaportas et al. - 2012 - Joint Specification of Model Space and Parameter S.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/EZK9TWMF/1207.html:text/html},
}

@article{lindley_statistical_1957,
	title = {A {Statistical} {Paradox}},
	volume = {44},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/44.1-2.187},
	doi = {10.1093/biomet/44.1-2.187},
	language = {en},
	number = {1-2},
	urldate = {2022-08-02},
	journal = {Biometrika},
	author = {Lindley, D. V.},
	year = {1957},
	pages = {187--192},
}

@incollection{smith_bayesian_1992,
	address = {Dordrecht},
	title = {Bayesian {Mixture} {Modeling}},
	isbn = {978-90-481-4220-0 978-94-017-2219-3},
	url = {http://link.springer.com/10.1007/978-94-017-2219-3_14},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Maximum {Entropy} and {Bayesian} {Methods}},
	publisher = {Springer Netherlands},
	author = {Neal, Radford M.},
	editor = {Smith, C. Ray and Erickson, Gary J. and Neudorfer, Paul O.},
	year = {1992},
	doi = {10.1007/978-94-017-2219-3_14},
	pages = {197--211},
}

@article{svensen_robust_2005,
	title = {Robust {Bayesian} mixture modelling},
	volume = {64},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231204005181},
	doi = {10.1016/j.neucom.2004.11.018},
	language = {en},
	urldate = {2022-08-03},
	journal = {Neurocomputing},
	author = {Svensén, Markus and Bishop, Christopher M.},
	month = mar,
	year = {2005},
	pages = {235--252},
}

@article{west_mixture_1993,
	title = {Mixture models, {Monte} {Carlo}, {Bayesian} updating and dynamic models},
	volume = {24},
	journal = {Computing Science and Statistics},
	author = {West, Mike},
	year = {1993},
	pages = {325 -- 333},
}

@article{gerlach_efficient_2000,
	title = {Efficient {Bayesian} {Inference} for {Dynamic} {Mixture} {Models}},
	volume = {95},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2000.10474273},
	doi = {10.1080/01621459.2000.10474273},
	language = {en},
	number = {451},
	urldate = {2022-08-03},
	journal = {Journal of the American Statistical Association},
	author = {Gerlach, Richard and Carter, Chris and Kohn, Robert},
	month = sep,
	year = {2000},
	pages = {819--828},
}

@book{garnier_viridis_2021,
	title = {viridis - {Colorblind}-{Friendly} {Color} {Maps} for {R}},
	url = {https://sjmgarnier.github.io/viridis/},
	author = {{Garnier} and {Simon} and {Ross} and {Noam} and {Rudis} and {Robert} and {Camargo} and Pedro, Antônio and {Sciaini} and {Marco} and {Scherer} and {Cédric}},
	year = {2021},
	doi = {10.5281/zenodo.4679424},
	doi = {10.5281/zenodo.4679424},
}

@article{aitchison_statistical_1982,
	title = {The {Statistical} {Analysis} of {Compositional} {Data}},
	volume = {44},
	number = {2},
	journal = {Journal of the Royal Statistical Society},
	author = {Aitchison, J.},
	year = {1982},
	pages = {139--177},
}

@book{sisson_handbook_2018,
	address = {Boca Raton, Florida : CRC Press, [2019]},
	edition = {1},
	title = {Handbook of {Approximate} {Bayesian} {Computation}},
	isbn = {978-1-315-11719-5},
	url = {https://www.taylorfrancis.com/books/9781439881514},
	language = {en},
	urldate = {2022-08-03},
	publisher = {Chapman and Hall/CRC},
	editor = {Sisson, S. A. and Fan, Y. and Beaumont, M. A.},
	month = sep,
	year = {2018},
	doi = {10.1201/9781315117195},
}

@article{shiffrin_survey_2008,
	title = {A {Survey} of {Model} {Evaluation} {Approaches} {With} a {Tutorial} on {Hierarchical} {Bayesian} {Methods}},
	volume = {32},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1080/03640210802414826},
	doi = {10.1080/03640210802414826},
	language = {en},
	number = {8},
	urldate = {2022-08-03},
	journal = {Cognitive Science},
	author = {Shiffrin, Richard M. and Lee, Michael D. and Kim, Woojae and Wagenmakers, Eric-Jan},
	month = dec,
	year = {2008},
	pages = {1248--1284},
}

@book{inselberg_parallel_2009,
	address = {New York, NY},
	title = {Parallel {Coordinates}},
	isbn = {978-0-387-21507-5 978-0-387-68628-8},
	url = {http://link.springer.com/10.1007/978-0-387-68628-8},
	language = {en},
	urldate = {2022-08-08},
	publisher = {Springer New York},
	author = {Inselberg, Alfred},
	year = {2009},
	doi = {10.1007/978-0-387-68628-8},
}

@incollection{klinger_parallel_1991,
	address = {Boston, MA},
	title = {Parallel {Coordinates}},
	isbn = {978-1-4684-5885-5 978-1-4684-5883-1},
	url = {http://link.springer.com/10.1007/978-1-4684-5883-1_9},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Human-{Machine} {Interactive} {Systems}},
	publisher = {Springer US},
	author = {Inselberg, Alfred and Dimsdale, Bernard},
	editor = {Klinger, Allen},
	year = {1991},
	doi = {10.1007/978-1-4684-5883-1_9},
	pages = {199--233},
}

@article{indratmo_efficacy_2018,
	title = {The efficacy of stacked bar charts in supporting single-attribute and overall-attribute comparisons},
	volume = {2},
	issn = {2468502X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468502X18300287},
	doi = {10.1016/j.visinf.2018.09.002},
	language = {en},
	number = {3},
	urldate = {2022-08-08},
	journal = {Visual Informatics},
	author = {{Indratmo} and Howorko, Lee and Boedianto, Joyce Maria and Daniel, Ben},
	month = sep,
	year = {2018},
	pages = {155--165},
}

@article{he_optimal_2022,
	title = {Optimal layout of stacked graph for visualizing multidimensional financial time series data},
	volume = {21},
	issn = {1473-8716, 1473-8724},
	url = {http://journals.sagepub.com/doi/10.1177/14738716211045005},
	doi = {10.1177/14738716211045005},
	abstract = {In the era of big data, the analysis of multi-dimensional time series data is one of the important topics in many fields such as finance, science, logistics, and engineering. Using stacked graphs for visual analysis helps to visually reveal the changing characteristics of each dimension over time. In order to present visually appealing and easy-to-read stacked graphs, this paper constructs the minimum cumulative variance rule to determine the stacking order of each dimension, as well as adopts the width priority principle and the color complementary principle to determine the label placement positioning and text coloring. In addition, a color matching method is recommended by user study. The proposed optimal visual layout algorithm is applied to the visual analysis of actual multidimensional financial time series data, and as a result, vividly reveals the characteristics of the flow of securities trading funds between sectors.},
	language = {en},
	number = {1},
	urldate = {2022-08-08},
	journal = {Information Visualization},
	author = {He, Yutian and Li, Hongjun},
	month = jan,
	year = {2022},
	pages = {63--73},
}

@article{byron_stacked_2008,
	title = {Stacked {Graphs} – {Geometry} \& {Aesthetics}},
	volume = {14},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/4658136/},
	doi = {10.1109/TVCG.2008.166},
	number = {6},
	urldate = {2022-08-08},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Byron, L. and Wattenberg, M.},
	month = nov,
	year = {2008},
	pages = {1245--1252},
}

@incollection{basu_aesthetics_2021,
	address = {Cham},
	title = {Aesthetics and {Ordering} in {Stacked} {Area} {Charts}},
	volume = {12909},
	isbn = {978-3-030-86061-5 978-3-030-86062-2},
	url = {https://link.springer.com/10.1007/978-3-030-86062-2_1},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Diagrammatic {Representation} and {Inference}},
	publisher = {Springer International Publishing},
	author = {Strunge Mathiesen, Steffen and Schulz, Hans-Jörg},
	editor = {Basu, Amrita and Stapleton, Gem and Linker, Sven and Legg, Catherine and Manalo, Emmanuel and Viana, Petrucio},
	year = {2021},
	doi = {10.1007/978-3-030-86062-2_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--19},
}

@article{daly_hodgkinhuxley_2015,
	title = {Hodgkin–{Huxley} revisited: reparametrization and identifiability analysis of the classic action potential model with approximate {Bayesian} methods},
	volume = {2},
	issn = {2054-5703},
	shorttitle = {Hodgkin–{Huxley} revisited},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.150499},
	doi = {10.1098/rsos.150499},
	abstract = {As cardiac cell models become increasingly complex, a correspondingly complex ‘genealogy’ of inherited parameter values has also emerged. The result has been the loss of a direct link between model parameters and experimental data, limiting both reproducibility and the ability to re-fit to new data. We examine the ability of approximate Bayesian computation (ABC) to infer parameter distributions in the seminal action potential model of Hodgkin and Huxley, for which an immediate and documented connection to experimental results exists. The ability of ABC to produce tight posteriors around the reported values for the gating rates of sodium and potassium ion channels validates the precision of this early work, while the highly variable posteriors around certain voltage dependency parameters suggests that voltage clamp experiments alone are insufficient to constrain the full model. Despite this, Hodgkin and Huxley's estimates are shown to be competitive with those produced by ABC, and the variable behaviour of posterior parametrized models under complex voltage protocols suggests that with additional data the model could be fully constrained. This work will provide the starting point for a full identifiability analysis of commonly used cardiac models, as well as a template for informative, data-driven parametrization of newly proposed models.},
	language = {en},
	number = {12},
	urldate = {2022-08-10},
	journal = {Royal Society Open Science},
	author = {Daly, Aidan C. and Gavaghan, David J. and Holmes, Chris and Cooper, Jonathan},
	month = dec,
	year = {2015},
	pages = {150499},
	file = {Full Text:/Users/marvin/Zotero/storage/48FWTWUQ/Daly et al. - 2015 - Hodgkin–Huxley revisited reparametrization and id.pdf:application/pdf},
}

@article{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {0022-3751, 1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1952.sp004764},
	doi = {10.1113/jphysiol.1952.sp004764},
	language = {en},
	number = {4},
	urldate = {2022-08-10},
	journal = {The Journal of Physiology},
	author = {Hodgkin, A. L. and Huxley, A. F.},
	month = aug,
	year = {1952},
	pages = {500--544},
	file = {Full Text:/Users/marvin/Zotero/storage/8RMC5X6R/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf:application/pdf},
}

@article{vavoulis_self-organizing_2012,
	title = {A {Self}-{Organizing} {State}-{Space}-{Model} {Approach} for {Parameter} {Estimation} in {Hodgkin}-{Huxley}-{Type} {Models} of {Single} {Neurons}},
	volume = {8},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1002401},
	doi = {10.1371/journal.pcbi.1002401},
	language = {en},
	number = {3},
	urldate = {2022-08-10},
	journal = {PLoS Computational Biology},
	author = {Vavoulis, Dimitrios V. and Straub, Volko A. and Aston, John A. D. and Feng, Jianfeng},
	editor = {Graham, Lyle J.},
	month = mar,
	year = {2012},
	pages = {e1002401},
	file = {Full Text:/Users/marvin/Zotero/storage/69N6BDUN/Vavoulis et al. - 2012 - A Self-Organizing State-Space-Model Approach for P.pdf:application/pdf},
}

@article{pospischil_minimal_2008,
	title = {Minimal {Hodgkin}–{Huxley} type models for different classes of cortical and thalamic neurons},
	volume = {99},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-008-0263-8},
	doi = {10.1007/s00422-008-0263-8},
	language = {en},
	number = {4-5},
	urldate = {2022-08-10},
	journal = {Biological Cybernetics},
	author = {Pospischil, Martin and Toledo-Rodriguez, Maria and Monier, Cyril and Piwkowska, Zuzanna and Bal, Thierry and Frégnac, Yves and Markram, Henry and Destexhe, Alain},
	month = nov,
	year = {2008},
	pages = {427--441},
}

@article{kass_bayes_1995,
	title = {Bayes {Factors}},
	volume = {90},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
	doi = {10.1080/01621459.1995.10476572},
	language = {en},
	number = {430},
	urldate = {2022-08-11},
	journal = {Journal of the American Statistical Association},
	author = {Kass, Robert E. and Raftery, Adrian E.},
	month = jun,
	year = {1995},
	pages = {773--795},
}

@article{kitchin_small_2015,
	title = {Small data in the era of big data},
	volume = {80},
	issn = {0343-2521, 1572-9893},
	url = {http://link.springer.com/10.1007/s10708-014-9601-7},
	doi = {10.1007/s10708-014-9601-7},
	language = {en},
	number = {4},
	urldate = {2022-08-11},
	journal = {GeoJournal},
	author = {Kitchin, Rob and Lauriault, Tracey P.},
	month = aug,
	year = {2015},
	pages = {463--475},
}

@article{morey_bayes_2011,
	title = {Bayes factor approaches for testing interval null hypotheses.},
	volume = {16},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024377},
	doi = {10.1037/a0024377},
	language = {en},
	number = {4},
	urldate = {2022-08-11},
	journal = {Psychological Methods},
	author = {Morey, Richard D. and Rouder, Jeffrey N.},
	month = dec,
	year = {2011},
	pages = {406--419},
}

@article{parr_computational_2018,
	title = {Computational {Neuropsychology} and {Bayesian} {Inference}},
	volume = {12},
	issn = {1662-5161},
	url = {http://journal.frontiersin.org/article/10.3389/fnhum.2018.00061/full},
	doi = {10.3389/fnhum.2018.00061},
	urldate = {2022-08-11},
	journal = {Frontiers in Human Neuroscience},
	author = {Parr, Thomas and Rees, Geraint and Friston, Karl J.},
	month = feb,
	year = {2018},
	pages = {61},
	file = {Full Text:/Users/marvin/Zotero/storage/22YSB4MU/Parr et al. - 2018 - Computational Neuropsychology and Bayesian Inferen.pdf:application/pdf},
}

@article{keysers_using_2020,
	title = {Using {Bayes} factor hypothesis testing in neuroscience to establish evidence of absence},
	volume = {23},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-020-0660-4},
	doi = {10.1038/s41593-020-0660-4},
	language = {en},
	number = {7},
	urldate = {2022-08-11},
	journal = {Nature Neuroscience},
	author = {Keysers, Christian and Gazzola, Valeria and Wagenmakers, Eric-Jan},
	month = jul,
	year = {2020},
	pages = {788--799},
	file = {Full Text:/Users/marvin/Zotero/storage/WQCPNRIB/Keysers et al. - 2020 - Using Bayes factor hypothesis testing in neuroscie.pdf:application/pdf},
}

@article{genell_model_2010,
	title = {Model selection in {Medical} {Research}: {A} simulation study comparing {Bayesian} {Model} {Averaging} and {Stepwise} {Regression}},
	volume = {10},
	issn = {1471-2288},
	shorttitle = {Model selection in {Medical} {Research}},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-10-108},
	doi = {10.1186/1471-2288-10-108},
	language = {en},
	number = {1},
	urldate = {2022-08-11},
	journal = {BMC Medical Research Methodology},
	author = {Genell, Anna and Nemes, Szilard and Steineck, Gunnar and Dickman, Paul W},
	month = dec,
	year = {2010},
	pages = {108},
	file = {Full Text:/Users/marvin/Zotero/storage/2AJX2TSB/Genell et al. - 2010 - Model selection in Medical Research A simulation .pdf:application/pdf},
}

@article{ashby_bayesian_2006,
	title = {Bayesian statistics in medicine: a 25 year review: {BAYESIAN} {STATISTICS} {IN} {MEDICINE}},
	volume = {25},
	issn = {02776715},
	shorttitle = {Bayesian statistics in medicine},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.2672},
	doi = {10.1002/sim.2672},
	language = {en},
	number = {21},
	urldate = {2022-08-11},
	journal = {Statistics in Medicine},
	author = {Ashby, Deborah},
	month = nov,
	year = {2006},
	pages = {3589--3631},
	file = {Full Text:/Users/marvin/Zotero/storage/PTY86GM3/Ashby - 2006 - Bayesian statistics in medicine a 25 year review.pdf:application/pdf},
}

@incollection{brown_coefficient_1998,
	address = {Berlin, Heidelberg},
	title = {Coefficient of {Variation}},
	isbn = {978-3-642-80330-7 978-3-642-80328-4},
	url = {http://link.springer.com/10.1007/978-3-642-80328-4_13},
	language = {en},
	urldate = {2022-08-11},
	booktitle = {Applied {Multivariate} {Statistics} in {Geohydrology} and {Related} {Sciences}},
	publisher = {Springer Berlin Heidelberg},
	author = {Brown, Charles E.},
	collaborator = {Brown, Charles E.},
	year = {1998},
	doi = {10.1007/978-3-642-80328-4_13},
	pages = {155--157},
}

@article{pandis_sampling_2015,
	title = {The sampling distribution},
	volume = {147},
	issn = {08895406},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0889540615000190},
	doi = {10.1016/j.ajodo.2015.01.009},
	language = {en},
	number = {4},
	urldate = {2022-08-11},
	journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
	author = {Pandis, Nikolaos},
	month = apr,
	year = {2015},
	pages = {517--519},
}

@article{kulesa_sampling_2015,
	title = {Sampling distributions and the bootstrap},
	volume = {12},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/nmeth.3414},
	doi = {10.1038/nmeth.3414},
	language = {en},
	number = {6},
	urldate = {2022-08-11},
	journal = {Nature Methods},
	author = {Kulesa, Anthony and Krzywinski, Martin and Blainey, Paul and Altman, Naomi},
	month = jun,
	year = {2015},
	pages = {477--478},
	file = {Full Text:/Users/marvin/Zotero/storage/XVRBX2YC/Kulesa et al. - 2015 - Sampling distributions and the bootstrap.pdf:application/pdf},
}

@incollection{lovric_skewness_2011,
	address = {Berlin, Heidelberg},
	title = {Skewness},
	isbn = {978-3-642-04897-5 978-3-642-04898-2},
	url = {http://link.springer.com/10.1007/978-3-642-04898-2_525},
	language = {en},
	urldate = {2022-08-12},
	booktitle = {International {Encyclopedia} of {Statistical} {Science}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hippel, Paul von},
	editor = {Lovric, Miodrag},
	year = {2011},
	doi = {10.1007/978-3-642-04898-2_525},
	pages = {1340--1342},
}

@book{bernardo_bayesian_2009,
	title = {Bayesian {Theory}},
	isbn = {978-0-470-31771-6},
	url = {https://nbn-resolving.org/urn:nbn:de:101:1-201412293383},
	language = {English},
	urldate = {2022-08-12},
	author = {Bernardo, Jos?? M and Smith, Adrian F. M},
	year = {2009},
	note = {OCLC: 899181890},
}

@article{yao_using_2018,
	title = {Using {Stacking} to {Average} {Bayesian} {Predictive} {Distributions} (with {Discussion})},
	volume = {13},
	issn = {1936-0975},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Using-Stacking-to-Average-Bayesian-Predictive-Distributions-with-Discussion/10.1214/17-BA1091.full},
	doi = {10.1214/17-BA1091},
	number = {3},
	urldate = {2022-08-12},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	month = sep,
	year = {2018},
	file = {Full Text:/Users/marvin/Zotero/storage/SQWHUZA9/Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:application/pdf},
}

@article{deng_generalized_2015,
	title = {Generalized evidence theory},
	volume = {43},
	issn = {0924-669X, 1573-7497},
	url = {http://link.springer.com/10.1007/s10489-015-0661-2},
	doi = {10.1007/s10489-015-0661-2},
	language = {en},
	number = {3},
	urldate = {2022-08-12},
	journal = {Applied Intelligence},
	author = {Deng, Yong},
	month = oct,
	year = {2015},
	pages = {530--543},
	file = {Submitted Version:/Users/marvin/Zotero/storage/R5BAPIBB/Deng - 2015 - Generalized evidence theory.pdf:application/pdf},
}

@misc{liang_enhancing_2020,
	title = {Enhancing {The} {Reliability} of {Out}-of-distribution {Image} {Detection} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.02690},
	abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7\% to 4.3\% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95\%.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
	month = aug,
	year = {2020},
	note = {arXiv:1706.02690 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/WYU6W5N7/Liang et al. - 2020 - Enhancing The Reliability of Out-of-distribution I.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/ZIP4QXYM/1706.html:text/html},
}

@misc{hendrycks_baseline_2018,
	title = {A {Baseline} for {Detecting} {Misclassified} and {Out}-of-{Distribution} {Examples} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1610.02136},
	abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = oct,
	year = {2018},
	note = {arXiv:1610.02136 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/UJATZ8JE/Hendrycks and Gimpel - 2018 - A Baseline for Detecting Misclassified and Out-of-.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/QDRWWVX2/1610.html:text/html},
}

@misc{devries_learning_2018,
	title = {Learning {Confidence} for {Out}-of-{Distribution} {Detection} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1802.04865},
	abstract = {Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = feb,
	year = {2018},
	note = {arXiv:1802.04865 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/THDMN2ES/DeVries and Taylor - 2018 - Learning Confidence for Out-of-Distribution Detect.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/ZZNKWRQS/1802.html:text/html},
}

@misc{schmitt_detecting_2022,
	title = {Detecting {Model} {Misspecification} in {Amortized} {Bayesian} {Inference} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/2112.08866},
	abstract = {Recent advances in probabilistic deep learning enable amortized Bayesian inference in settings where the likelihood function is implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? In this paper, we conceptualize the types of model misspecification arising in simulation-based inference and systematically investigate the performance of SNPE-C (APT) and the BayesFlow framework under these misspecifications. We propose an augmented optimization objective which imposes a probabilistic structure on the learned latent data summary space and utilize maximum mean discrepancy (MMD) to detect potentially catastrophic misspecifications during inference undermining the validity of the obtained results. We verify our detection criterion on a number of artificial and realistic misspecifications, ranging from toy conjugate models to complex models of decision making and disease outbreak dynamics applied to real data. Further, we show that posterior inference errors increase when the distance between the latent summary distributions of the true data-generating process and the training simulations grows. Thus, we demonstrate the dual utility of MMD as a method for detecting model misspecification and as a proxy for verifying the faithfulness of amortized simulation-based Bayesian inference.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Schmitt, Marvin and Bürkner, Paul-Christian and Köthe, Ullrich and Radev, Stefan T.},
	month = may,
	year = {2022},
	note = {arXiv:2112.08866 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/A49JESQ7/Schmitt et al. - 2022 - Detecting Model Misspecification in Amortized Baye.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/Q892U27C/2112.html:text/html},
}

@book{shafer_mathematical_1976,
	address = {Princeton, NJ},
	title = {A mathematical theory of evidence},
	isbn = {978-0-691-10042-5},
	language = {eng},
	publisher = {Princeton Univ. Press},
	author = {Shafer, Glenn},
	year = {1976},
	file = {Table of Contents PDF:/Users/marvin/Zotero/storage/UFLQMQ7A/Shafer - 1976 - A mathematical theory of evidence.pdf:application/pdf},
}

@book{gelman_bayesian_2014,
	address = {Boca Raton},
	edition = {Third edition},
	series = {Chapman \& {Hall}/{CRC} texts in statistical science},
	title = {Bayesian data analysis},
	isbn = {978-1-4398-4095-5},
	publisher = {CRC Press},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David and Vehtari, Aki and Rubin, Donald B},
	year = {2014},
	keywords = {Bayesian statistical decision theory, MATHEMATICS / Probability \& Statistics / General},
}

@techreport{kolczynska_modeling_2020,
	type = {preprint},
	title = {Modeling public opinion over time and space: {Trust} in state institutions in {Europe}, 1989-2019},
	shorttitle = {Modeling public opinion over time and space},
	url = {https://osf.io/3v5g7},
	abstract = {The dynamics of political trust remain understudied, largely due to challenges associated with the comparability of data from different survey projects. Relying on Bayesian multilevel Item Response Theory models applied to data from 13 cross-national survey projects, we estimated levels of political trust in 27 European countries between 1989 and 2019. To improve the quality of estimates and correct for the differences in survey sample representativeness, we applied poststratification by sex, age, and education. We describe trajectories of political trust in Europe finding support for the thesis about trendless fluctuations in trust rather than any clear long-term tendency, with substantial differences between countries and European regions. In line with prior research, we find larger differences in political trust by education level than by sex or age.},
	urldate = {2022-08-15},
	institution = {SocArXiv},
	author = {Kolczynska, Marta and Bürkner, Paul - Christian and Kennedy, Lauren and Vehtari, Aki},
	month = aug,
	year = {2020},
	doi = {10.31235/osf.io/3v5g7},
	file = {Submitted Version:/Users/marvin/Zotero/storage/UNR64C6P/Kolczynska et al. - 2020 - Modeling public opinion over time and space Trust.pdf:application/pdf},
}

@article{rigdon_bayesian_2009,
	title = {A {Bayesian} {Prediction} {Model} for the {U}.{S}. {Presidential} {Election}},
	volume = {37},
	issn = {1532-673X, 1552-3373},
	url = {http://journals.sagepub.com/doi/10.1177/1532673X08330670},
	doi = {10.1177/1532673X08330670},
	abstract = {It has become a popular pastime for political pundits and scholars alike to predict the winner of the U.S. presidential election. Although forecasting has now quite a history, we argue that the closeness of recent presidential elections and the wide accessibility of data should change how presidential election forecasting is conducted. We present a Bayesian forecasting model that concentrates on the Electoral College outcome and considers finer details such as third-party candidates and self-proclaimed undecided voters. We incorporate our estimators into a dynamic programming algorithm to determine the probability that a candidate will win an election.},
	language = {en},
	number = {4},
	urldate = {2022-08-15},
	journal = {American Politics Research},
	author = {Rigdon, Steven E. and Jacobson, Sheldon H. and Tam Cho, Wendy K. and Sewell, Edward C. and Rigdon, Christopher J.},
	month = jul,
	year = {2009},
	pages = {700--724},
}

@article{heidemanns_updated_2020,
	title = {An {Updated} {Dynamic} {Bayesian} {Forecasting} {Model} for the {US} {Presidential} {Election}},
	volume = {2},
	url = {https://hdsr.mitpress.mit.edu/pub/nw1dzd02},
	doi = {10.1162/99608f92.fc62f1e1},
	language = {en},
	number = {4},
	urldate = {2022-08-15},
	journal = {Harvard Data Science Review},
	author = {Heidemanns, Merlin and Gelman, Andrew and Morris, G. Elliott},
	month = oct,
	year = {2020},
	file = {Full Text:/Users/marvin/Zotero/storage/J3FPVVHR/Heidemanns et al. - 2020 - An Updated Dynamic Bayesian Forecasting Model for .pdf:application/pdf},
}

@article{linzer_dynamic_2013,
	title = {Dynamic {Bayesian} {Forecasting} of {Presidential} {Elections} in the {States}},
	volume = {108},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2012.737735},
	doi = {10.1080/01621459.2012.737735},
	language = {en},
	number = {501},
	urldate = {2022-08-15},
	journal = {Journal of the American Statistical Association},
	author = {Linzer, Drew A.},
	month = mar,
	year = {2013},
	pages = {124--134},
}

@article{lock_bayesian_2010,
	title = {Bayesian {Combination} of {State} {Polls} and {Election} {Forecasts}},
	volume = {18},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/product/identifier/S104719870001250X/type/journal_article},
	doi = {10.1093/pan/mpq002},
	abstract = {A wide range of potentially useful data are available for election forecasting: the results of previous elections, a multitude of preelection polls, and predictors such as measures of national and statewide economic performance. How accurate are different forecasts? We estimate predictive uncertainty via analysis of data collected from past elections (actual outcomes, preelection polls, and model estimates). With these estimated uncertainties, we use Bayesian inference to integrate the various sources of data to form posterior distributions for the state and national two-party Democratic vote shares for the 2008 election. Our key idea is to separately forecast the national popular vote shares and the relative positions of the states. More generally, such an approach could be applied to study changes in public opinion and other phenomena with wide national swings and fairly stable spatial distributions relative to the national average.},
	language = {en},
	number = {3},
	urldate = {2022-08-15},
	journal = {Political Analysis},
	author = {Lock, Kari and Gelman, Andrew},
	year = {2010},
	pages = {337--348},
	file = {Full Text:/Users/marvin/Zotero/storage/SJRYIP3V/Lock and Gelman - 2010 - Bayesian Combination of State Polls and Election F.pdf:application/pdf},
}

@article{ellison_introduction_1996,
	title = {An {Introduction} to {Bayesian} {Inference} for {Ecological} {Research} and {Environmental} {Decision} {Making}},
	volume = {6},
	number = {4},
	journal = {Ecological Applications},
	author = {Ellison, Aaron M},
	year = {1996},
	pages = {1036--1046},
}

@article{frost_interpreting_2021,
	title = {Interpreting the results of clinical trials, embracing uncertainty: {A} {Bayesian} approach},
	volume = {65},
	issn = {0001-5172, 1399-6576},
	shorttitle = {Interpreting the results of clinical trials, embracing uncertainty},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/aas.13725},
	doi = {10.1111/aas.13725},
	language = {en},
	number = {2},
	urldate = {2022-08-15},
	journal = {Acta Anaesthesiologica Scandinavica},
	author = {Frost, Steven A. and Alexandrou, Evan and Schulz, Luis and Aneman, Anders},
	month = feb,
	year = {2021},
	pages = {146--150},
}

@book{shaddick_spatio-temporal_2016,
	title = {Spatio-temporal methods in environmental epidemiology},
	isbn = {978-1-4822-3704-7 978-0-429-16294-7},
	url = {http://www.crcnetbase.com/isbn/9781482237047},
	abstract = {"Spatio-Temporal Methods in Environmental Epidemiology is the first book of its kind to specifically address the interface between environmental epidemiology and spatio-temporal modeling. In response to the growing need for collaboration between statisticians and environmental epidemiologists, the book links recent developments in spatio-temporal methodology with epidemiological applications. Drawing on real-life problems, it provides the necessary tools to exploit advances in methodology when assessing the health risks associated with environmental hazards. The book\{u2019\}s clear guidelines enable the implementation of the methodology and estimation of risks in practice. Designed for graduate students in both epidemiology and statistics, the text covers a wide range of topics, from an introduction to epidemiological principles and the foundations of spatio-temporal modeling to new research directions. It describes traditional and Bayesian approaches and presents the theory of spatial, temporal, and spatio-temporal modeling in the context of its application to environmental epidemiology. The text includes practical examples together with embedded R code, details of specific R packages, and the use of other software, such as WinBUGS/OpenBUGS and integrated nested Laplace approximations (INLA). A supplementary website provides additional code, data, examples, exercises, lab projects, and more."--Publisher's description.},
	language = {English},
	urldate = {2022-08-15},
	author = {Shaddick, Gavin and Zidek, James V},
	year = {2016},
	note = {OCLC: 958799792},
}

@article{anderson_embracing_1998,
	title = {Embracing {Uncertainty}: {The} {Interface} of {Bayesian} {Statistics} and {Cognitive} {Psychology}},
	volume = {2},
	number = {1},
	journal = {Conservation Ecology},
	author = {Anderson, Judith L},
	year = {1998},
	pages = {Article 2 (online)},
}

@article{efron_why_1986,
	title = {Why {Isn}'t {Everyone} a {Bayesian}?},
	volume = {40},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1986.10475342},
	doi = {10.1080/00031305.1986.10475342},
	number = {1},
	journal = {The American Statistician},
	author = {Efron, B.},
	year = {1986},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1986.10475342},
	pages = {1--5},
}

@misc{moore_cramming_1965,
	title = {Cramming more components onto integrated circuits},
	url = {https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf},
	urldate = {2022-08-15},
	publisher = {Electronics Magazine},
	author = {Moore, Gordon E},
	year = {1965},
}

@article{green_bayesian_2015,
	title = {Bayesian computation: a summary of the current state, and samples backwards and forwards},
	volume = {25},
	issn = {0960-3174, 1573-1375},
	shorttitle = {Bayesian computation},
	url = {http://link.springer.com/10.1007/s11222-015-9574-5},
	doi = {10.1007/s11222-015-9574-5},
	language = {en},
	number = {4},
	urldate = {2022-08-15},
	journal = {Statistics and Computing},
	author = {Green, Peter J. and Łatuszyński, Krzysztof and Pereyra, Marcelo and Robert, Christian P.},
	month = jul,
	year = {2015},
	pages = {835--862},
	file = {Full Text:/Users/marvin/Zotero/storage/DR78JW3I/Green et al. - 2015 - Bayesian computation a summary of the current sta.pdf:application/pdf},
}

@article{robert_short_2011,
	title = {A {Short} {History} of {Markov} {Chain} {Monte} {Carlo}: {Subjective} {Recollections} from {Incomplete} {Data}},
	volume = {26},
	issn = {0883-4237},
	shorttitle = {A {Short} {History} of {Markov} {Chain} {Monte} {Carlo}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-26/issue-1/A-Short-History-of-Markov-Chain-Monte-Carlo--Subjective/10.1214/10-STS351.full},
	doi = {10.1214/10-STS351},
	number = {1},
	urldate = {2022-08-15},
	journal = {Statistical Science},
	author = {Robert, Christian and Casella, George},
	month = feb,
	year = {2011},
	file = {Full Text:/Users/marvin/Zotero/storage/E3844MB5/Robert and Casella - 2011 - A Short History of Markov Chain Monte Carlo Subje.pdf:application/pdf},
}

@article{wagenmakers_bayesian_2010,
	title = {Bayesian hypothesis testing for psychologists: {A} tutorial on the {Savage}–{Dickey} method},
	volume = {60},
	issn = {00100285},
	shorttitle = {Bayesian hypothesis testing for psychologists},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028509000826},
	doi = {10.1016/j.cogpsych.2009.12.001},
	language = {en},
	number = {3},
	urldate = {2022-08-15},
	journal = {Cognitive Psychology},
	author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
	month = may,
	year = {2010},
	pages = {158--189},
	file = {Full Text:/Users/marvin/Zotero/storage/TT8QQ9DQ/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:application/pdf},
}

@incollection{griffiths_bayesian_2001,
	edition = {1},
	title = {Bayesian {Models} of {Cognition}},
	isbn = {978-0-511-81677-2 978-0-521-85741-3 978-0-521-67410-2},
	url = {https://www.cambridge.org/core/product/identifier/9780511816772%23c85741-ch3/type/book_part},
	urldate = {2022-08-15},
	booktitle = {The {Cambridge} {Handbook} of {Computational} {Psychology}},
	publisher = {Cambridge University Press},
	author = {Griffiths, Thomas L. and Kemp, Charles and Tenenbaum, Joshua B.},
	month = jan,
	year = {2001},
	doi = {10.1017/CBO9780511816772.006},
	pages = {59--100},
	file = {Submitted Version:/Users/marvin/Zotero/storage/YIYRFTY7/Griffiths et al. - 2001 - Bayesian Models of Cognition.pdf:application/pdf},
}

@article{etz_introduction_2018,
	title = {Introduction to {Bayesian} {Inference} for {Psychology}},
	volume = {25},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-017-1262-3},
	doi = {10.3758/s13423-017-1262-3},
	language = {en},
	number = {1},
	urldate = {2022-08-15},
	journal = {Psychonomic Bulletin \& Review},
	author = {Etz, Alexander and Vandekerckhove, Joachim},
	month = feb,
	year = {2018},
	pages = {5--34},
	file = {Full Text:/Users/marvin/Zotero/storage/PHJBYGVX/Etz and Vandekerckhove - 2018 - Introduction to Bayesian Inference for Psychology.pdf:application/pdf},
}

@incollection{wasif_bayesian_2020,
	title = {Bayesian {Analyses} of {Political} {Decision} {Making}},
	isbn = {978-0-19-022863-7},
	url = {https://oxfordre.com/politics/view/10.1093/acrefore/9780190228637.001.0001/acrefore-9780190228637-e-1002},
	abstract = {Bayes’ theorem is a relatively simple equation but one of the most important mathematical principles discovered. It is a formalization of a basic cognitive process: updating expectations as new information is obtained. It was derived from the laws of conditional probability by Reverend Thomas Bayes and published posthumously in 1763. In the 21st century, it is used in academic fields ranging from computer science to social science.
            The theorem’s most prominent use is in statistical inference. In this regard, there are three essential tenets of Bayesian thought that distinguish it from standard approaches. First, any quantity that is not known as an absolute fact is treated probabilistically, meaning that a numerical probability or a probability distribution is assigned. Second, research questions and designs are based on prior knowledge and expressed as prior distributions. Finally, these prior distributions are updated by conditioning on new data through the use of Bayes’ theorem to create a posterior distribution that is a compromise between prior and data knowledge.
            This approach has a number of advantages, especially in social science. First, it gives researchers the probability of observing the parameter given the data, which is the inverse of the results from frequentist inference and more appropriate for social scientific data and parameters. Second, Bayesian approaches excel at estimating parameters for complex data structures and functional forms, and provide more information about these parameters compared to standard approaches. This is possible due to stochastic simulation techniques called Markov Chain Monte Carlo. Third, Bayesian approaches allow for the explicit incorporation of previous estimates through the use of the prior distribution. This provides a formal mechanism for incorporating previous estimates and a means of comparing potential results.
            Bayes’ theorem is also used in machine learning, which is a subset of computer science that focuses on algorithms that learn from data to make predictions. One such algorithm is the Naive Bayes Classifier, which uses Bayes’ theorem to classify objects such as documents based on prior relationships. Bayesian networks can be seen as a complicated version of the Naive Classifier that maps, estimates, and predicts relationships in a network. It is useful for more complicated prediction problems. Lastly, the theorem has even been used by qualitative social scientists as a formal mechanism for stating and evaluating beliefs and updating knowledge.},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Oxford {Research} {Encyclopedia} of {Politics}},
	publisher = {Oxford University Press},
	author = {Wasif, Kumail and Gill, Jeff},
	collaborator = {Wasif, Kumail and Gill, Jeff},
	month = oct,
	year = {2020},
	doi = {10.1093/acrefore/9780190228637.013.1002},
}

@book{lindley_bayesian_1972,
	title = {Bayesian {Statistics}: {A} {Review}},
	isbn = {978-0-89871-002-1},
	shorttitle = {Bayesian {Statistics}},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611970654},
	language = {en},
	urldate = {2022-08-15},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Lindley, D. V.},
	month = jan,
	year = {1972},
	doi = {10.1137/1.9781611970654},
}

@article{neyman_frequentist_1977,
	title = {Frequentist probability and frequentist statistics},
	volume = {36},
	issn = {0039-7857, 1573-0964},
	url = {http://link.springer.com/10.1007/BF00485695},
	doi = {10.1007/BF00485695},
	language = {en},
	number = {1},
	urldate = {2022-08-15},
	journal = {Synthese},
	author = {Neyman, J.},
	month = sep,
	year = {1977},
	pages = {97--131},
}

@article{lindley_future_1975,
	title = {The {Future} of {Statistics}: {A} {Bayesian} 21st {Century}},
	volume = {7},
	issn = {00018678},
	shorttitle = {The {Future} of {Statistics}},
	url = {http://www.jstor.org/stable/1426315?origin=crossref},
	doi = {10.2307/1426315},
	urldate = {2022-08-15},
	journal = {Advances in Applied Probability},
	author = {Lindley, D. V.},
	month = sep,
	year = {1975},
	pages = {106},
}

@article{mollick_establishing_2006,
	title = {Establishing {Moore}'s {Law}},
	volume = {28},
	issn = {1058-6180},
	url = {http://ieeexplore.ieee.org/document/1677462/},
	doi = {10.1109/MAHC.2006.45},
	language = {en},
	number = {3},
	urldate = {2022-08-15},
	journal = {IEEE Annals of the History of Computing},
	author = {Mollick, E.},
	month = jul,
	year = {2006},
	pages = {62--75},
}

@article{shalf_future_2020,
	title = {The future of computing beyond {Moore}’s {Law}},
	volume = {378},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0061},
	doi = {10.1098/rsta.2019.0061},
	abstract = {Moore’s Law is a techno-economic model that has enabled the information technology industry to double the performance and functionality of digital electronics roughly every 2 years within a fixed cost, power and area. Advances in silicon lithography have enabled this exponential miniaturization of electronics, but, as transistors reach atomic scale and fabrication costs continue to rise, the classical technological driver that has underpinned Moore’s Law for 50 years is failing and is anticipated to flatten by 2025. This article provides an updated view of what a post-exascale system will look like and the challenges ahead, based on our most recent understanding of technology roadmaps. It also discusses the tapering of historical improvements, and how it affects options available to continue scaling of successors to the first exascale machine. Lastly, this article covers the many different opportunities and strategies available to continue computing performance improvements in the absence of historical technology drivers.
            This article is part of a discussion meeting issue ‘Numerical algorithms for high-performance computational science’.},
	language = {en},
	number = {2166},
	urldate = {2022-08-15},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Shalf, John},
	month = mar,
	year = {2020},
	pages = {20190061},
	file = {Full Text:/Users/marvin/Zotero/storage/2YHNEQKA/Shalf - 2020 - The future of computing beyond Moore’s Law.pdf:application/pdf},
}

@article{hadlak_survey_2015,
	title = {A {Survey} of {Multi}-faceted {Graph} {Visualization}},
	url = {https://diglib.eg.org/handle/10.2312/eurovisstar.20151109.001-020},
	doi = {10.2312/EUROVISSTAR.20151109},
	abstract = {Graph visualization is an important field in information visualization that is centered on the graphical display of graph-structured data. Yet real world data is rarely just graph-structured, but instead exhibits multiple facets, such as multivariate attributes, or spatial and temporal frames of reference. In an effort to display different facets of a graph, such a wealth of visualization techniques has been developed in the past that current surveys focus on a single additional facet only in order to enumerate and classify them. This report builds on existing graph visualization surveys for the four common facets of partitions, attributes, time, and space. It contributes a generic high-level categorization of faceted graph visualization that subsumes the existing classifications, which can be understood as facet-specific refinements of the resulting categories. Furthermore, it extends beyond existing surveys by applying the same categorization to graph visualizations with multiple facets. For each of the introduced categories and considered facets, this overview provides visualization examples to illustrate instances of their realization.},
	urldate = {2022-08-15},
	journal = {Eurographics Conference on Visualization (EuroVis) - STARs},
	author = {Hadlak, Steffen and Schumann, Heidrun and Schulz, Hans-Jörg},
	year = {2015},
	note = {Artwork Size: 20 pages
ISBN: 9783905674996
Publisher: The Eurographics Association},
	keywords = {I.3.3 [Computer Graphics], Line and curve generation, Picture/Image Generation},
	pages = {20 pages},
}

@article{pang_approaches_1997,
	title = {Approaches to uncertainty visualization},
	volume = {13},
	issn = {01782789},
	url = {http://link.springer.com/10.1007/s003710050111},
	doi = {10.1007/s003710050111},
	number = {8},
	urldate = {2022-08-15},
	journal = {The Visual Computer},
	author = {Pang, Alex T. and Wittenbrink, Craig M. and Lodha, Suresh K.},
	month = nov,
	year = {1997},
	pages = {370--390},
}

@article{mcgill_variations_1978,
	title = {Variations of {Box} {Plots}},
	volume = {32},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1978.10479236},
	doi = {10.1080/00031305.1978.10479236},
	number = {1},
	journal = {The American Statistician},
	author = {Mcgill, Robert and Tukey, John W. and Larsen, Wayne A.},
	year = {1978},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1978.10479236},
	pages = {12--16},
}

@book{tukey_exploratory_1977,
	address = {Reading, Mass},
	series = {Addison-{Wesley} series in behavioral science},
	title = {Exploratory data analysis},
	isbn = {978-0-201-07616-5},
	publisher = {Addison-Wesley Pub. Co},
	author = {Tukey, John Wilder},
	year = {1977},
	keywords = {Statistics},
}

@article{bordoloi_visualization_2004,
	title = {Visualization techniques for spatial probability density function data},
	volume = {3},
	issn = {1683-1470},
	url = {http://datascience.codata.org/articles/abstract/10.2481/dsj.3.153/},
	doi = {10.2481/dsj.3.153},
	language = {en},
	urldate = {2022-08-15},
	journal = {Data Science Journal},
	author = {Bordoloi, Udeepta D. and Kao, David L. and Shen, Han-Wei},
	year = {2004},
	pages = {153--162},
	file = {Full Text:/Users/marvin/Zotero/storage/KRKUE3SP/Bordoloi et al. - 2004 - Visualization techniques for spatial probability d.pdf:application/pdf},
}

@techreport{wickham_40_2012,
	title = {40 years of boxplots},
	institution = {had.co.nz},
	author = {Wickham, Hadley and Stryjewski, Lisa},
	year = {2012},
}

@article{anscombe_graphs_1973,
	title = {Graphs in {Statistical} {Analysis}},
	volume = {27},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1973.10478966},
	doi = {10.1080/00031305.1973.10478966},
	language = {en},
	number = {1},
	urldate = {2022-08-15},
	journal = {The American Statistician},
	author = {Anscombe, F. J.},
	month = feb,
	year = {1973},
	pages = {17--21},
}

@article{kampstra_beanplot_2008,
	title = {Beanplot: {A} {Boxplot} {Alternative} for {Visual} {Comparison} of {Distributions}},
	volume = {28},
	issn = {1548-7660},
	shorttitle = {Beanplot},
	url = {http://www.jstatsoft.org/v28/c01/},
	doi = {10.18637/jss.v028.c01},
	language = {en},
	number = {Code Snippet 1},
	urldate = {2022-08-15},
	journal = {Journal of Statistical Software},
	author = {Kampstra, Peter},
	year = {2008},
	file = {Submitted Version:/Users/marvin/Zotero/storage/RH2TA9LB/Kampstra - 2008 - Beanplot A Boxplot Alternative for Visual Compari.pdf:application/pdf},
}

@article{hintze_violin_1998,
	title = {Violin {Plots}: {A} {Box} {Plot}-{Density} {Trace} {Synergism}},
	volume = {52},
	issn = {0003-1305, 1537-2731},
	shorttitle = {Violin {Plots}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1998.10480559},
	doi = {10.1080/00031305.1998.10480559},
	language = {en},
	number = {2},
	urldate = {2022-08-15},
	journal = {The American Statistician},
	author = {Hintze, Jerry L. and Nelson, Ray D.},
	month = may,
	year = {1998},
	pages = {181--184},
}

@article{phong_illumination_1975,
	title = {Illumination for computer generated pictures},
	volume = {18},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/360825.360839},
	doi = {10.1145/360825.360839},
	abstract = {The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
	language = {en},
	number = {6},
	urldate = {2022-08-15},
	journal = {Communications of the ACM},
	author = {Phong, Bui Tuong},
	month = jun,
	year = {1975},
	pages = {311--317},
	file = {Full Text:/Users/marvin/Zotero/storage/VZXWC5ZL/Phong - 1975 - Illumination for computer generated pictures.pdf:application/pdf},
}

@book{balakrishnan_primer_2003,
	address = {Hoboken, N.J},
	title = {A primer on statistical distributions},
	isbn = {978-0-471-42798-8},
	publisher = {Wiley},
	author = {Balakrishnan, N. and Nevzorov, V. B.},
	year = {2003},
	keywords = {Distribution (Probability theory)},
}

@book{hausser_entropy_2021,
	title = {entropy: {Estimation} of {Entropy}, {Mutual} {Information} and {Related} {Quantities}},
	url = {https://CRAN.R-project.org/package=entropy},
	author = {Hausser, Jean and Strimmer, Korbinian},
	year = {2021},
}

@article{shannon_mathematical_1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	volume = {27},
	issn = {00058580},
	url = {https://ieeexplore.ieee.org/document/6773024},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	language = {en},
	number = {3},
	urldate = {2022-08-16},
	journal = {Bell System Technical Journal},
	author = {Shannon, C. E.},
	month = jul,
	year = {1948},
	pages = {379--423},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/RR25B5VF/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/DUXKCJLH/1412.html:text/html},
}

@article{k_hackenberger_bayes_2019,
	title = {Bayes or not {Bayes}, is this the question?},
	volume = {60},
	issn = {0353-9504, 1332-8166},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6406060/},
	doi = {10.3325/cmj.2019.60.50},
	number = {1},
	urldate = {2022-08-18},
	journal = {Croatian Medical Journal},
	author = {K. Hackenberger, Branimir},
	month = feb,
	year = {2019},
	pages = {50--52},
	file = {Full Text:/Users/marvin/Zotero/storage/G9JQCAVW/K. Hackenberger - 2019 - Bayes or not Bayes, is this the question.pdf:application/pdf},
}

@book{rossi_mathematical_2018,
	address = {Hoboken, NJ},
	edition = {1st edition},
	title = {Mathematical statistics: an introduction to likelihood based inference},
	isbn = {978-1-118-77116-7 978-1-118-77097-9},
	shorttitle = {Mathematical statistics},
	publisher = {John Wiley \& Sons},
	author = {Rossi, Richard J.},
	year = {2018},
	keywords = {Mathematical statistics},
}

@article{pernet_null_2016,
	title = {Null hypothesis significance testing: a short tutorial},
	volume = {4},
	issn = {2046-1402},
	shorttitle = {Null hypothesis significance testing},
	url = {https://f1000research.com/articles/4-621/v3},
	doi = {10.12688/f1000research.6963.3},
	abstract = {Although thoroughly criticized, null hypothesis significance testing (NHST) remains the statistical method of choice used to provide evidence for an effect, in biological, biomedical and social sciences. In this short tutorial, I first summarize the concepts behind the method, distinguishing test of significance (Fisher) and test of acceptance (Newman-Pearson) and point to common interpretation errors regarding the p-value. I then present the related concepts of confidence intervals and again point to common interpretation errors. Finally, I discuss what should be reported in which context. The goal is to clarify concepts to avoid interpretation errors and propose reporting practices.},
	language = {en},
	urldate = {2022-08-18},
	journal = {F1000Research},
	author = {Pernet, Cyril},
	month = oct,
	year = {2016},
	pages = {621},
	file = {Full Text:/Users/marvin/Zotero/storage/WUN3P8IC/Pernet - 2016 - Null hypothesis significance testing a short tuto.pdf:application/pdf},
}

@article{neyman_frequentist_1977-1,
	title = {Frequentist {Probability} and {Frequentist} {Statistics}},
	volume = {36},
	issn = {00397857, 15730964},
	url = {http://www.jstor.org/stable/20115217},
	number = {1},
	urldate = {2022-08-18},
	journal = {Synthese},
	author = {Neyman, J.},
	year = {1977},
	note = {Publisher: Springer},
	pages = {97--131},
}

@misc{aguilar_intuitive_2022,
	title = {Intuitive {Joint} {Priors} for {Bayesian} {Linear} {Multilevel} {Models}: {The} {R2}-{D2}-{M2} prior},
	shorttitle = {Intuitive {Joint} {Priors} for {Bayesian} {Linear} {Multilevel} {Models}},
	url = {http://arxiv.org/abs/2208.07132},
	abstract = {The training of high-dimensional regression models on comparably sparse data is an important yet complicated topic, especially when there are many more model parameters than observations in the data. From a Bayesian perspective, inference in such cases can be achieved with the help of shrinkage prior distributions, at least for generalized linear models. However, real-world data usually possess multilevel structures, such as repeated measurements or natural groupings of individuals, which existing shrinkage priors are not built to deal with. We generalize and extend one of these priors, the R2-D2 prior by Zhang et al. (2020), to linear multilevel models leading to what we call the R2-D2-M2 prior. The proposed prior enables both local and global shrinkage of the model parameters. It comes with interpretable hyperparameters, which we show to be intrinsically related to vital properties of the prior, such as rates of concentration around the origin, tail behavior, and amount of shrinkage the prior exerts. We offer guidelines on how to select the prior's hyperparameters by deriving shrinkage factors and measuring the effective number of non-zero model coefficients. Hence, the user can readily evaluate and interpret the amount of shrinkage implied by a specific choice of hyperparameters. Finally, we perform extensive experiments on simulated and real data, showing that our prior is well calibrated, has desirable global and local regularization properties and enables the reliable and interpretable estimation of much more complex Bayesian multilevel models than was previously possible.},
	urldate = {2022-08-18},
	publisher = {arXiv},
	author = {Aguilar, Javier Enrique and Bürkner, Paul-Christian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.07132 [stat]},
	keywords = {Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/6BDRH9UJ/Aguilar and Bürkner - 2022 - Intuitive Joint Priors for Bayesian Linear Multile.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/GUKF226Z/2208.html:text/html},
}

@article{howarth_sources_1996,
	title = {Sources for a history of the ternary diagram},
	volume = {29},
	issn = {0007-0874, 1474-001X},
	url = {https://www.cambridge.org/core/product/identifier/S000708740003449X/type/journal_article},
	doi = {10.1017/S000708740003449X},
	abstract = {Anyone reading the literature on the history of graphs will soon realize that the use of graphie displays of any type was really quite unusual until the mid-ninetenth century and that those scientists who did make use of them are often familiar to us as creative thinkers in their own fields of endeavour. A
              ternary diagram
              (also known as a
              triangular diagram
              ) is a particular type of graph which consists of an equilateral triangle in which a given plotted point represents the relative proportions (
              a, b, c
              ) of three end-members (A, B and C), generally expressed as percentages and constrained by
              a + b + c
              = 100\%. It has long been used to portray sample composition in terms of three constituents, or an observed colour in terms of three primary colours, because it is a convenient means of representing a three-component System in a planar projection, rather than as an isometric, or similar, view of a three-dimensional space. Recent papers suggest that its use is not as familiar to some statisticians as are other commonly used forms of graph. For example, although it was cited by Peddle in 1910 and more recently by Dickinson, it is not discussed in modern texts on statistical graphies nor in the key papers on the history of graphs. However, beginning with studies of colour-mixing in the eighteenth century, it has subsequently become widely used, particularly in geology, physical chemistry and metallurgy. In this paper, I attempt to document its gradual uptake as a standard method of data display and some of the scientific advances which its use has facilitated.},
	language = {en},
	number = {3},
	urldate = {2022-08-19},
	journal = {The British Journal for the History of Science},
	author = {Howarth, Richard J.},
	month = sep,
	year = {1996},
	pages = {337--356},
}

@book{safarevic_linear_2013,
	address = {Berlin New York},
	title = {Linear algebra and geometry},
	isbn = {978-3-642-30994-6},
	language = {eng},
	publisher = {Springer},
	author = {Šafarevič, Igorʹ Rostislavovič and Remizov, Alexey O.},
	year = {2013},
}

@article{buba-brzozowa_cevas_2000,
	title = {Ceva's and {Menelaus}' {Theorems} for the n-{Dimensional} {Space}},
	volume = {4},
	number = {2},
	journal = {Journal for Geometry and Graphics},
	author = {Buba-Brzozowa, Malgorzata},
	year = {2000},
	pages = {115--118},
}

@book{johnson_univariate_1992,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and mathematical statistics},
	title = {Univariate discrete distributions},
	isbn = {978-0-471-54897-3},
	publisher = {Wiley},
	author = {Johnson, Norman Lloyd and Kotz, Samuel and Kemp, Adrienne W. and Johnson, Norman Lloyd},
	year = {1992},
	keywords = {Distribution (Probability theory)},
}

@article{liu_zero-and-one_2018,
	title = {A zero-and-one inflated {Poisson} model and its application},
	volume = {11},
	issn = {19387989, 19387997},
	url = {http://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0011/0002/a011/},
	doi = {10.4310/SII.2018.v11.n2.a11},
	language = {en},
	number = {2},
	urldate = {2022-08-24},
	journal = {Statistics and Its Interface},
	author = {Liu, Wenchen and Tang, Yincai and Xu, Ancha},
	year = {2018},
	pages = {339--351},
}

@article{hartwig_median_2020,
	title = {The median and the mode as robust meta‐analysis estimators in the presence of small‐study effects and outliers},
	volume = {11},
	issn = {1759-2879, 1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1402},
	doi = {10.1002/jrsm.1402},
	language = {en},
	number = {3},
	urldate = {2022-08-24},
	journal = {Research Synthesis Methods},
	author = {Hartwig, Fernando P. and Davey Smith, George and Schmidt, Amand F. and Sterne, Jonathan A. C. and Higgins, Julian P. T. and Bowden, Jack},
	month = may,
	year = {2020},
	pages = {397--412},
	file = {Full Text:/Users/marvin/Zotero/storage/VKQTY3XE/Hartwig et al. - 2020 - The median and the mode as robust meta‐analysis es.pdf:application/pdf},
}

@article{muller_possible_2000,
	title = {Possible advantages of a robust evaluation of comparisons},
	volume = {105},
	issn = {1044677X},
	url = {https://nvlpubs.nist.gov/nistpubs/jres/105/4/j54mul.pdf},
	doi = {10.6028/jres.105.044},
	number = {4},
	urldate = {2022-08-24},
	journal = {Journal of Research of the National Institute of Standards and Technology},
	author = {Muller, J.W.},
	month = jul,
	year = {2000},
	pages = {551},
	file = {Full Text:/Users/marvin/Zotero/storage/A53VZLVS/Muller - 2000 - Possible advantages of a robust evaluation of comp.pdf:application/pdf},
}

@article{aitchison_general_1985,
	title = {A {General} {Class} of {Distributions} on the {Simplex}},
	volume = {47},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1985.tb01341.x},
	doi = {10.1111/j.2517-6161.1985.tb01341.x},
	language = {en},
	number = {1},
	urldate = {2022-10-04},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Aitchison, J.},
	month = sep,
	year = {1985},
	pages = {136--146},
}

@article{lodewyckx_tutorial_2011-1,
	title = {A tutorial on {Bayes} factor estimation with the product space method},
	volume = {55},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249611000423},
	doi = {10.1016/j.jmp.2011.06.001},
	language = {en},
	number = {5},
	urldate = {2022-10-05},
	journal = {Journal of Mathematical Psychology},
	author = {Lodewyckx, Tom and Kim, Woojae and Lee, Michael D. and Tuerlinckx, Francis and Kuppens, Peter and Wagenmakers, Eric-Jan},
	month = oct,
	year = {2011},
	pages = {331--347},
	file = {Full Text:/Users/marvin/Zotero/storage/K4EPUN6Z/Lodewyckx et al. - 2011 - A tutorial on Bayes factor estimation with the pro.pdf:application/pdf},
}

@article{rossell_concentration_2022,
	title = {Concentration of {Posterior} {Model} {Probabilities} and {Normalized} {L0} {Criteria}},
	volume = {17},
	issn = {1936-0975},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-17/issue-2/Concentration-of-Posterior-Model-Probabilities-and-Normalized-L0-Criteria/10.1214/21-BA1262.full},
	doi = {10.1214/21-BA1262},
	number = {2},
	urldate = {2022-10-05},
	journal = {Bayesian Analysis},
	author = {Rossell, David},
	month = jun,
	year = {2022},
	file = {Full Text:/Users/marvin/Zotero/storage/7BIIQGL2/Rossell - 2022 - Concentration of Posterior Model Probabilities and.pdf:application/pdf},
}

@article{yang_bayesian_2018,
	title = {Bayesian selection of misspecified models is overconfident and may cause spurious posterior probabilities for phylogenetic trees},
	volume = {115},
	url = {https://pnas.org/doi/full/10.1073/pnas.1712673115},
	doi = {10.1073/pnas.1712673115},
	abstract = {Significance
            The Bayesian method is widely used to estimate species phylogenies using molecular sequence data. While it has long been noted to produce spuriously high posterior probabilities for trees or clades, the precise reasons for this overconfidence are unknown. Here we characterize the behavior of Bayesian model selection when the compared models are misspecified and demonstrate that when the models are nearly equally wrong, the method exhibits unpleasant polarized behaviors, supporting one model with high confidence while rejecting others. This provides an explanation for the empirical observation of spuriously high posterior probabilities in molecular phylogenetics.
          , 
            The Bayesian method is noted to produce spuriously high posterior probabilities for phylogenetic trees in analysis of large datasets, but the precise reasons for this overconfidence are unknown. In general, the performance of Bayesian selection of misspecified models is poorly understood, even though this is of great scientific interest since models are never true in real data analysis. Here we characterize the asymptotic behavior of Bayesian model selection and show that when the competing models are equally wrong, Bayesian model selection exhibits surprising and polarized behaviors in large datasets, supporting one model with full force while rejecting the others. If one model is slightly less wrong than the other, the less wrong model will eventually win when the amount of data increases, but the method may become overconfident before it becomes reliable. We suggest that this extreme behavior may be a major factor for the spuriously high posterior probabilities for evolutionary trees. The philosophical implications of our results to the application of Bayesian model selection to evaluate opposing scientific hypotheses are yet to be explored, as are the behaviors of non-Bayesian methods in similar situations.},
	language = {en},
	number = {8},
	urldate = {2022-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Ziheng and Zhu, Tianqi},
	month = feb,
	year = {2018},
	pages = {1854--1859},
	file = {Full Text:/Users/marvin/Zotero/storage/7HZDV9H3/Yang and Zhu - 2018 - Bayesian selection of misspecified models is overc.pdf:application/pdf},
}

@misc{stan_development_team_prior_2020,
	title = {Prior {Choice} {Recommendations}},
	url = {https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations},
	author = {{Stan Development Team}},
	year = {2020},
}

@article{grinsztajn_bayesian_2021,
	title = {Bayesian workflow for disease transmission modeling in {Stan}},
	volume = {40},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.9164},
	doi = {10.1002/sim.9164},
	language = {en},
	number = {27},
	urldate = {2022-10-10},
	journal = {Statistics in Medicine},
	author = {Grinsztajn, Léo and Semenova, Elizaveta and Margossian, Charles C. and Riou, Julien},
	month = nov,
	year = {2021},
	pages = {6209--6234},
	file = {Full Text:/Users/marvin/Zotero/storage/57JKWARM/Grinsztajn et al. - 2021 - Bayesian workflow for disease transmission modelin.pdf:application/pdf},
}

@article{pospischil_comparison_2011,
	title = {Comparison of different neuron models to conductance-based post-stimulus time histograms obtained in cortical pyramidal cells using dynamic-clamp in vitro},
	volume = {105},
	number = {2},
	journal = {Biological cybernetics},
	author = {Pospischil, Martin and Piwkowska, Zuzanna and Bal, Thierry and Destexhe, Alain},
	year = {2011},
	note = {Publisher: Springer},
	pages = {167},
}

@misc{burkner_models_2022,
	title = {Some models are useful, but how do we know which ones? {Towards} a unified {Bayesian} model taxonomy},
	shorttitle = {Some models are useful, but how do we know which ones?},
	url = {http://arxiv.org/abs/2209.02439},
	abstract = {Probabilistic (Bayesian) modeling has experienced a surge of applications in almost all quantitative sciences and industrial areas. This development is driven by a combination of several factors, including better probabilistic estimation algorithms, flexible software, increased computing power, and a growing awareness of the benefits of probabilistic learning. However, a principled Bayesian model building workflow is far from complete and many challenges remain. To aid future research and applications of a principled Bayesian workflow, we ask and provide answers for what we perceive as two fundamental questions of Bayesian modeling, namely (a) "What actually is a Bayesian model?" and (b) "What makes a good Bayesian model?". As an answer to the first question, we propose the PAD model taxonomy that defines four basic kinds of Bayesian models, each representing some combination of the assumed joint distribution of all (known or unknown) variables (P), a posterior approximator (A), and training data (D). As an answer to the second question, we propose ten utility dimensions according to which we can evaluate Bayesian models holistically, namely, (1) causal consistency, (2) parameter recoverability, (3) predictive performance, (4) fairness, (5) structural faithfulness, (6) parsimony, (7) interpretability, (8) convergence, (9) estimation speed, and (10) robustness. Further, we propose two example utility decision trees that describe hierarchies and trade-offs between utilities depending on the inferential goals that drive model building and testing.},
	urldate = {2022-10-11},
	publisher = {arXiv},
	author = {Bürkner, Paul-Christian and Scholz, Maximilian and Radev, Stefan T.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.02439 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/EBDBFTH2/Bürkner et al. - 2022 - Some models are useful, but how do we know which o.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/JJRMUM43/2209.html:text/html},
}

@article{salvatier_probabilistic_2016,
	title = {Probabilistic programming in {Python} using {PyMC3}},
	volume = {2},
	journal = {PeerJ Computer Science},
	author = {Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher},
	year = {2016},
	note = {Publisher: PeerJ Inc.},
	pages = {e55},
}

@article{barron_consistency_1999,
	title = {The consistency of posterior distributions in nonparametric problems},
	volume = {27},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-27/issue-2/The-consistency-of-posterior-distributions-in-nonparametric-problems/10.1214/aos/1018031206.full},
	doi = {10.1214/aos/1018031206},
	number = {2},
	urldate = {2022-10-12},
	journal = {The Annals of Statistics},
	author = {Barron, Andrew and Schervish, Mark J. and Wasserman, Larry},
	month = apr,
	year = {1999},
	file = {Full Text:/Users/marvin/Zotero/storage/M6J7VY4Y/Barron et al. - 1999 - The consistency of posterior distributions in nonp.pdf:application/pdf},
}

@book{van_der_vaart_asymptotic_2000,
	title = {Asymptotic statistics},
	volume = {3},
	publisher = {Cambridge university press},
	author = {van der Vaart, Aad W},
	year = {2000},
}

@misc{cannon_investigating_2022,
	title = {Investigating the {Impact} of {Model} {Misspecification} in {Neural} {Simulation}-based {Inference}},
	url = {http://arxiv.org/abs/2209.01845},
	abstract = {Aided by advances in neural density estimation, considerable progress has been made in recent years towards a suite of simulation-based inference (SBI) methods capable of performing flexible, black-box, approximate Bayesian inference for stochastic simulation models. While it has been demonstrated that neural SBI methods can provide accurate posterior approximations, the simulation studies establishing these results have considered only well-specified problems -- that is, where the model and the data generating process coincide exactly. However, the behaviour of such algorithms in the case of model misspecification has received little attention. In this work, we provide the first comprehensive study of the behaviour of neural SBI algorithms in the presence of various forms of model misspecification. We find that misspecification can have a profoundly deleterious effect on performance. Some mitigation strategies are explored, but no approach tested prevents failure in all cases. We conclude that new approaches are required to address model misspecification if neural SBI algorithms are to be relied upon to derive accurate scientific conclusions.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Cannon, Patrick and Ward, Daniel and Schmon, Sebastian M.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01845 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/UWN94SGD/Cannon et al. - 2022 - Investigating the Impact of Model Misspecification.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/9VAAAA43/2209.html:text/html},
}

@misc{leclercq_simulation-based_2022,
	title = {Simulation-based inference of {Bayesian} hierarchical models while checking for model misspecification},
	url = {http://arxiv.org/abs/2209.11057},
	abstract = {This paper presents recent methodological advances to perform simulation-based inference (SBI) of a general class of Bayesian hierarchical models (BHMs), while checking for model misspecification. Our approach is based on a two-step framework. First, the latent function that appears as second layer of the BHM is inferred and used to diagnose possible model misspecification. Second, target parameters of the trusted model are inferred via SBI. Simulations used in the first step are recycled for score compression, which is necessary to the second step. As a proof of concept, we apply our framework to a prey-predator model built upon the Lotka-Volterra equations and involving complex observational processes.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Leclercq, Florent},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11057 [astro-ph, q-bio, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Statistics - Methodology, Astrophysics - Instrumentation and Methods for Astrophysics, Quantitative Biology - Populations and Evolution},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/RSTMJJ2R/Leclercq - 2022 - Simulation-based inference of Bayesian hierarchica.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/2LVVDHQB/2209.html:text/html},
}

@misc{schmon_generalized_2021,
	title = {Generalized {Posteriors} in {Approximate} {Bayesian} {Computation}},
	url = {http://arxiv.org/abs/2011.08644},
	abstract = {Complex simulators have become a ubiquitous tool in many scientific disciplines, providing high-fidelity, implicit probabilistic models of natural and social phenomena. Unfortunately, they typically lack the tractability required for conventional statistical analysis. Approximate Bayesian computation (ABC) has emerged as a key method in simulation-based inference, wherein the true model likelihood and posterior are approximated using samples from the simulator. In this paper, we draw connections between ABC and generalized Bayesian inference (GBI). First, we re-interpret the accept/reject step in ABC as an implicitly defined error model. We then argue that these implicit error models will invariably be misspecified. While ABC posteriors are often treated as a necessary evil for approximating the standard Bayesian posterior, this allows us to re-interpret ABC as a potential robustification strategy. This leads us to suggest the use of GBI within ABC, a use case we explore empirically.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Schmon, Sebastian M. and Cannon, Patrick W. and Knoblauch, Jeremias},
	month = feb,
	year = {2021},
	note = {arXiv:2011.08644 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/9ZYPYWRE/Schmon et al. - 2021 - Generalized Posteriors in Approximate Bayesian Com.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/92SRRKRQ/2011.html:text/html},
}

@misc{matsubara_robust_2022,
	title = {Robust {Generalised} {Bayesian} {Inference} for {Intractable} {Likelihoods}},
	url = {http://arxiv.org/abs/2104.07359},
	abstract = {Generalised Bayesian inference updates prior beliefs using a loss function, rather than a likelihood, and can therefore be used to confer robustness against possible mis-specification of the likelihood. Here we consider generalised Bayesian inference with a Stein discrepancy as a loss function, motivated by applications in which the likelihood contains an intractable normalisation constant. In this context, the Stein discrepancy circumvents evaluation of the normalisation constant and produces generalised posteriors that are either closed form or accessible using standard Markov chain Monte Carlo. On a theoretical level, we show consistency, asymptotic normality, and bias-robustness of the generalised posterior, highlighting how these properties are impacted by the choice of Stein discrepancy. Then, we provide numerical experiments on a range of intractable distributions, including applications to kernel-based exponential family models and non-Gaussian graphical models.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Matsubara, Takuo and Knoblauch, Jeremias and Briol, François-Xavier and Oates, Chris J.},
	month = jan,
	year = {2022},
	note = {arXiv:2104.07359 [math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/3JDZBN9J/Matsubara et al. - 2022 - Robust Generalised Bayesian Inference for Intracta.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/INRWMUQP/2104.html:text/html},
}

@article{dellaporta_robust_2022,
	title = {Robust {Bayesian} {Inference} for {Simulator}-based {Models} via the {MMD} {Posterior} {Bootstrap}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2202.04744},
	doi = {10.48550/ARXIV.2202.04744},
	abstract = {Simulator-based models are models for which the likelihood is intractable but simulation of synthetic data is possible. They are often used to describe complex real-world phenomena, and as such can often be misspecified in practice. Unfortunately, existing Bayesian approaches for simulators are known to perform poorly in those cases. In this paper, we propose a novel algorithm based on the posterior bootstrap and maximum mean discrepancy estimators. This leads to a highly-parallelisable Bayesian inference algorithm with strong robustness properties. This is demonstrated through an in-depth theoretical study which includes generalisation bounds and proofs of frequentist consistency and robustness of our posterior. The approach is then assessed on a range of examples including a g-and-k distribution and a toggle-switch model.},
	urldate = {2022-10-14},
	author = {Dellaporta, Charita and Knoblauch, Jeremias and Damoulas, Theodoros and Briol, François-Xavier},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Methodology (stat.ME), Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{pacchiardi_score_2022,
	title = {Score {Matched} {Neural} {Exponential} {Families} for {Likelihood}-{Free} {Inference}},
	url = {http://arxiv.org/abs/2012.10903},
	abstract = {Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distributions for stochastic models with intractable likelihood, by relying on model simulations. In Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to the observation in order to sample from an approximate posterior, whose form depends on the chosen statistics. In this work, we introduce a new way to learn ABC statistics: we first generate parameter-simulation pairs from the model independently on the observation; then, we use Score Matching to train a neural conditional exponential family to approximate the likelihood. The exponential family is the largest class of distributions with fixed-size sufficient statistics; thus, we use them in ABC, which is intuitively appealing and has state-of-the-art performance. In parallel, we insert our likelihood approximation in an MCMC for doubly intractable distributions to draw posterior samples. We can repeat that for any number of observations with no additional model simulations, with performance comparable to related approaches. We validate our methods on toy models with known likelihood and a large-dimensional time-series model.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Pacchiardi, Lorenzo and Dutta, Ritabrata},
	month = jan,
	year = {2022},
	note = {arXiv:2012.10903 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/XABNHKGX/Pacchiardi and Dutta - 2022 - Score Matched Neural Exponential Families for Like.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/8Y5HTM66/2012.html:text/html},
}

@inproceedings{ruff_deep_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {One}-{Class} {Classification}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/ruff18a.html},
	abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and Müller, Emmanuel and Kloft, Marius},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {4393--4402},
}

@article{pang_deep_2022,
	title = {Deep {Learning} for {Anomaly} {Detection}: {A} {Review}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning} for {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2007.02500},
	doi = {10.1145/3439950},
	abstract = {Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection, has emerged as a critical direction. This paper surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in three high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages and disadvantages, and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.},
	number = {2},
	urldate = {2022-10-14},
	journal = {ACM Computing Surveys},
	author = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton van den},
	month = mar,
	year = {2022},
	note = {arXiv:2007.02500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--38},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/TC7P5AGG/Pang et al. - 2022 - Deep Learning for Anomaly Detection A Review.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/YWFUW8JT/2007.html:text/html},
}

@misc{delaunoy_towards_2022,
	title = {Towards {Reliable} {Simulation}-{Based} {Inference} with {Balanced} {Neural} {Ratio} {Estimation}},
	url = {http://arxiv.org/abs/2208.13624},
	abstract = {Modern approaches for simulation-based inference rely upon deep learning surrogates to enable approximate inference with computer simulators. In practice, the estimated posteriors' computational faithfulness is, however, rarely guaranteed. For example, Hermans et al. (2021) show that current simulation-based inference algorithms can produce posteriors that are overconfident, hence risking false inferences. In this work, we introduce Balanced Neural Ratio Estimation (BNRE), a variation of the NRE algorithm designed to produce posterior approximations that tend to be more conservative, hence improving their reliability, while sharing the same Bayes optimal solution. We achieve this by enforcing a balancing condition that increases the quantified uncertainty in small simulation budget regimes while still converging to the exact posterior as the budget increases. We provide theoretical arguments showing that BNRE tends to produce posterior surrogates that are more conservative than NRE's. We evaluate BNRE on a wide variety of tasks and show that it produces conservative posterior surrogates on all tested benchmarks and simulation budgets. Finally, we emphasize that BNRE is straightforward to implement over NRE and does not introduce any computational overhead.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Delaunoy, Arnaud and Hermans, Joeri and Rozet, François and Wehenkel, Antoine and Louppe, Gilles},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13624 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/VHQB976T/Delaunoy et al. - 2022 - Towards Reliable Simulation-Based Inference with B.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/43ACWZ9R/2208.html:text/html},
}

@misc{ward_robust_2022,
	title = {Robust {Neural} {Posterior} {Estimation} and {Statistical} {Model} {Criticism}},
	url = {http://arxiv.org/abs/2210.06564},
	abstract = {Computer simulations have proven a valuable tool for understanding complex phenomena across the sciences. However, the utility of simulators for modelling and forecasting purposes is often restricted by low data quality, as well as practical limits to model fidelity. In order to circumvent these difficulties, we argue that modellers must treat simulators as idealistic representations of the true data generating process, and consequently should thoughtfully consider the risk of model misspecification. In this work we revisit neural posterior estimation (NPE), a class of algorithms that enable black-box parameter inference in simulation models, and consider the implication of a simulation-to-reality gap. While recent works have demonstrated reliable performance of these methods, the analyses have been performed using synthetic data generated by the simulator model itself, and have therefore only addressed the well-specified case. In this paper, we find that the presence of misspecification, in contrast, leads to unreliable inference when NPE is used naively. As a remedy we argue that principled scientific inquiry with simulators should incorporate a model criticism component, to facilitate interpretable identification of misspecification and a robust inference component, to fit 'wrong but useful' models. We propose robust neural posterior estimation (RNPE), an extension of NPE to simultaneously achieve both these aims, through explicitly modelling the discrepancies between simulations and the observed data. We assess the approach on a range of artificially misspecified examples, and find RNPE performs well across the tasks, whereas naively using NPE leads to misleading and erratic posteriors.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Ward, Daniel and Cannon, Patrick and Beaumont, Mark and Fasiolo, Matteo and Schmon, Sebastian M.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06564 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/IK6GCEP8/Ward et al. - 2022 - Robust Neural Posterior Estimation and Statistical.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/Z86S2W7G/2210.html:text/html},
}

@misc{mescheder_adversarial_2018,
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	shorttitle = {Adversarial {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1701.04722},
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = jun,
	year = {2018},
	note = {arXiv:1701.04722 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/LMENYH4Q/Mescheder et al. - 2018 - Adversarial Variational Bayes Unifying Variationa.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/3PS3WBJ3/1701.html:text/html},
}

@article{von_krause_mental_2022,
	title = {Mental speed is high until age 60 as revealed by analysis of over a million participants},
	volume = {6},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01282-7},
	doi = {10.1038/s41562-021-01282-7},
	language = {en},
	number = {5},
	urldate = {2022-10-14},
	journal = {Nature Human Behaviour},
	author = {von Krause, Mischa and Radev, Stefan T. and Voss, Andreas},
	month = may,
	year = {2022},
	pages = {700--708},
}

@techreport{ghaderi-kangavari_general_2022,
	type = {preprint},
	title = {A general integrative neurocognitive modeling framework to jointly describe {EEG} and decision-making on single trials},
	url = {https://osf.io/pqv2c},
	abstract = {Despite advances in techniques for exploring reciprocity in brain-behavior relations, few studies focus on building neurocognitive models that describe both human EEG and behavioral modalities at the single-trial level. Here, we introduce a new integrative joint modeling framework for the simultaneous description of single-trial EEG measures and cognitive modeling parameters of decision making. The new framework can be utilized for the evaluation of research questions as well as the prediction of both data types concurrently. In the introduced joint models, we formalized how single-trial N200 latencies and Centro-parietal positivities (CPP) are predicted by changing single-trial parameters of various drift-diffusion models (DDMs). These models do not have clear closed-form likelihoods and are not easy to fit using Markov chain Monte Carlo (MCMC) methods because nuisance parameters on single trials are shared in both behavior and neural activity. We trained deep neural networks to learn the Bayesian posterior distributions of unobserved neurocognitive parameters based on model simulations. We then used parameter recovery assessment and model misspecification to ascertain how robustly the models’ parameters can be estimated. Moreover, we fit the models to three different real datasets to test their applicability. Our results show that the single-trial integrative models can recover their latent parameters. Finally, we provide some pieces of evidence that single-trial integrative joint models are superior to traditional integrative models. The current single-trial paradigm and the likelihood-free approach for parameter recovery can inspire scientists and modelers to conveniently develop new neuro-cognitive models for other neural measures and to evaluate them appropriately.},
	urldate = {2022-10-14},
	institution = {PsyArXiv},
	author = {Ghaderi-Kangavari, Amin and Rad, Jamal Amani and Nunez, Michael D.},
	month = aug,
	year = {2022},
	doi = {10.31234/osf.io/pqv2c},
	file = {Submitted Version:/Users/marvin/Zotero/storage/CZIANQ2Z/Ghaderi-Kangavari et al. - 2022 - A general integrative neurocognitive modeling fram.pdf:application/pdf},
}

@inproceedings{lueckmann_benchmarking_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Benchmarking {Simulation}-{Based} {Inference}},
	volume = {130},
	url = {https://proceedings.mlr.press/v130/lueckmann21a.html},
	abstract = {Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such ’likelihood-free’ algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
	editor = {Banerjee, Arindam and Fukumizu, Kenji},
	month = apr,
	year = {2021},
	pages = {343--351},
}

@book{berger_likelihood_1988,
	address = {Hayward, Calif},
	edition = {2nd ed},
	series = {Lecture notes-monograph series},
	title = {The likelihood principle},
	isbn = {978-0-940600-13-3},
	number = {v. 6},
	publisher = {Institute of Mathematical Statistics},
	author = {Berger, James O. and Wolpert, Robert L.},
	year = {1988},
	keywords = {Mathematical statistics, Probabilities, Estimation theory},
}

@book{jombart_outbreaks_2020,
	title = {outbreaks: {A} {Collection} of {Disease} {Outbreak} {Data}},
	url = {https://CRAN.R-project.org/package=outbreaks},
	author = {Jombart, Thibaut and Frost, Simon and Nouvellet, Pierre and Campbell, Finlay and Sudre, Bertrand},
	year = {2020},
}

@article{jones-todd_identifying_2019,
	title = {Identifying prognostic structural features in tissue sections of colon cancer patients using point pattern analysis: {Point} pattern analysis of colon cancer tissue sections},
	volume = {38},
	issn = {02776715},
	shorttitle = {Identifying prognostic structural features in tissue sections of colon cancer patients using point pattern analysis},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.8046},
	doi = {10.1002/sim.8046},
	language = {en},
	number = {8},
	urldate = {2022-10-26},
	journal = {Statistics in Medicine},
	author = {Jones-Todd, Charlotte M. and Caie, Peter and Illian, Janine B. and Stevenson, Ben C. and Savage, Anne and Harrison, David J. and Bown, James L.},
	month = apr,
	year = {2019},
	pages = {1421--1441},
	file = {Accepted Version:/Users/marvin/Zotero/storage/A4VZ6536/Jones-Todd et al. - 2019 - Identifying prognostic structural features in tiss.pdf:application/pdf},
}

@misc{sailynoja_graphical_2021,
	title = {Graphical {Test} for {Discrete} {Uniformity} and its {Applications} in {Goodness} of {Fit} {Evaluation} and {Multiple} {Sample} {Comparison}},
	url = {http://arxiv.org/abs/2103.10522},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows.},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	month = nov,
	year = {2021},
	note = {arXiv:2103.10522 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/7767VHYE/Säilynoja et al. - 2021 - Graphical Test for Discrete Uniformity and its App.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/I8JRJL8V/2103.html:text/html},
}

@misc{talts_validating_2020-1,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv:1804.06788 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/UU66JETJ/Talts et al. - 2020 - Validating Bayesian Inference Algorithms with Simu.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/Y748743B/1804.html:text/html},
}

@article{nguyen_sensitivity_2019,
	title = {Sensitivity of {Optimal} {Estimation} satellite retrievals to misspecification of the prior mean and covariance, with application to {OCO}-2 retrievals},
	volume = {11},
	number = {23},
	journal = {Remote Sensing},
	author = {Nguyen, Hai and Cressie, Noel and Hobbs, Jonathan},
	year = {2019},
	note = {Publisher: MDPI},
	pages = {2770},
}

@misc{bergamin_model-agnostic_2022,
	title = {Model-agnostic out-of-distribution detection using combined statistical tests},
	url = {http://arxiv.org/abs/2203.01097},
	abstract = {We present simple methods for out-of-distribution detection using a trained generative model. These techniques, based on classical statistical tests, are model-agnostic in the sense that they can be applied to any differentiable generative model. The idea is to combine a classical parametric test (Rao's score test) with the recently introduced typicality test. These two test statistics are both theoretically well-founded and exploit different sources of information based on the likelihood for the typicality test and its gradient for the score test. We show that combining them using Fisher's method overall leads to a more accurate out-of-distribution test. We also discuss the benefits of casting out-of-distribution detection as a statistical testing problem, noting in particular that false positive rate control can be valuable for practical out-of-distribution detection. Despite their simplicity and generality, these methods can be competitive with model-specific out-of-distribution detection algorithms without any assumptions on the out-distribution.},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Bergamin, Federico and Mattei, Pierre-Alexandre and Havtorn, Jakob D. and Senetaire, Hugo and Schmutz, Hugo and Maaløe, Lars and Hauberg, Søren and Frellsen, Jes},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01097 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/M4N6B7YZ/Bergamin et al. - 2022 - Model-agnostic out-of-distribution detection using.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/S4VVG4M2/2203.html:text/html},
}

@book{neal_mcmc_2011,
	title = {{MCMC} using {Hamiltonian} dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	urldate = {2022-11-23},
	author = {Neal, Radford M.},
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
	note = {arXiv:1206.1901 [physics, stat]},
	keywords = {Statistics - Computation, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/marvin/Zotero/storage/QSSY4ZND/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:application/pdf;arXiv.org Snapshot:/Users/marvin/Zotero/storage/W7BG6N4A/1206.html:text/html},
}

@article{radev_jana_2023,
	title = {{JANA}: {Jointly} {Amortized} {Neural} {Approximation} of {Complex} {Bayesian} {Models}},
	journal = {arXiv preprint},
	author = {Radev, Stefan T and Schmitt, Marvin and Pratz, Valentin and Picchini, Umberto and Köthe, Ullrich and Bürkner, Paul-Christian},
	year = {2023},
}

@article{Greenacre2021,
  title = {Compositional Data Analysis},
  volume = {8},
  ISSN = {2326-831X},
  url = {http://dx.doi.org/10.1146/annurev-statistics-042720-124436},
  DOI = {10.1146/annurev-statistics-042720-124436},
  number = {1},
  journal = {Annual Review of Statistics and Its Application},
  publisher = {Annual Reviews},
  author = {Greenacre,  Michael},
  year = {2021},
  pages = {271–299}
}

@article{Welchman2016,
  title = {The Human Brain in Depth: How We See in 3D},
  volume = {2},
  ISSN = {2374-4650},
  url = {http://dx.doi.org/10.1146/annurev-vision-111815-114605},
  DOI = {10.1146/annurev-vision-111815-114605},
  number = {1},
  journal = {Annual Review of Vision Science},
  publisher = {Annual Reviews},
  author = {Welchman,  Andrew E.},
  year = {2016},
  pages = {345–376}
}

@article{Dumuid2020,
  title = {Compositional Data Analysis in Time-Use Epidemiology: What,  Why,  How},
  volume = {17},
  ISSN = {1660-4601},
  url = {http://dx.doi.org/10.3390/ijerph17072220},
  DOI = {10.3390/ijerph17072220},
  number = {7},
  journal = {International Journal of Environmental Research and Public Health},
  publisher = {MDPI AG},
  author = {Dumuid,  Dorothea and Pedišić,  Željko and Palarea-Albaladejo,  Javier and Martín-Fernández,  Josep Antoni and Hron,  Karel and Olds,  Timothy},
  year = {2020},
  month = mar,
  pages = {2220}
}

@inbook{PhilipDawid2011,
  title = {Posterior Model Probabilities},
  url = {http://dx.doi.org/10.1016/B978-0-444-51862-0.50019-8},
  DOI = {10.1016/b978-0-444-51862-0.50019-8},
  booktitle = {Philosophy of Statistics},
  publisher = {Elsevier},
  author = {Dawid,  Alexander Philip},
  year = {2011},
  pages = {607–630}
}


@InProceedings{schmitt2023metauncertainty,
  title = 	 {{Meta-Uncertainty in Bayesian Model Comparison}},
  author =       {Schmitt, Marvin and Radev, Stefan T. and B\"urkner, Paul-Christian},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {11--29},
  year = 	 {2023},
  volume = 	 {206},
  series = 	 {PMLR}
}

@article{Gloor2017,
  title = {Microbiome Datasets Are Compositional: And This Is Not Optional},
  volume = {8},
  ISSN = {1664-302X},
  url = {http://dx.doi.org/10.3389/fmicb.2017.02224},
  DOI = {10.3389/fmicb.2017.02224},
  journal = {Frontiers in Microbiology},
  publisher = {Frontiers Media SA},
  author = {Gloor,  Gregory B. and Macklaim,  Jean M. and Pawlowsky-Glahn,  Vera and Egozcue,  Juan J.},
  year = {2017},
}